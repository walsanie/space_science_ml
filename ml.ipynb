{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3c5902",
   "metadata": {},
   "source": [
    "# Practical: Predicting the Diameter of an Asteroid Using NN\n",
    "### Waleed Alsanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213661a",
   "metadata": {},
   "source": [
    "## 1. Loading and Pre-processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21592173",
   "metadata": {},
   "source": [
    "We want to some preprocessing of the data using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11a285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567efd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\AppData\\Local\\Temp\\ipykernel_19012\\1777567520.py:1: DtypeWarning: Columns (0,10,15,16,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"Asteroid_updated.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>...</th>\n",
       "      <th>UB</th>\n",
       "      <th>IR</th>\n",
       "      <th>spec_B</th>\n",
       "      <th>spec_T</th>\n",
       "      <th>G</th>\n",
       "      <th>moid</th>\n",
       "      <th>class</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ceres</td>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.594780</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pallas</td>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>34.836234</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>72318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.233240</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Juno</td>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>72684.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sk</td>\n",
       "      <td>S</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.034540</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vesta</td>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>24288.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.139480</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Astraea</td>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>63507.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.095890</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839709</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.812945</td>\n",
       "      <td>0.664688</td>\n",
       "      <td>4.695700</td>\n",
       "      <td>183.310012</td>\n",
       "      <td>234.618352</td>\n",
       "      <td>0.943214</td>\n",
       "      <td>4.682676</td>\n",
       "      <td>4.717914</td>\n",
       "      <td>17298.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032397</td>\n",
       "      <td>APO</td>\n",
       "      <td>0.208911</td>\n",
       "      <td>1723.217927</td>\n",
       "      <td>156.905910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839710</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.645238</td>\n",
       "      <td>0.259376</td>\n",
       "      <td>12.574937</td>\n",
       "      <td>1.620020</td>\n",
       "      <td>339.568072</td>\n",
       "      <td>1.959126</td>\n",
       "      <td>3.331350</td>\n",
       "      <td>4.302346</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956145</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.229090</td>\n",
       "      <td>1571.431965</td>\n",
       "      <td>13.366251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839711</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.373137</td>\n",
       "      <td>0.202053</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>176.499082</td>\n",
       "      <td>198.026527</td>\n",
       "      <td>1.893638</td>\n",
       "      <td>2.852636</td>\n",
       "      <td>3.655884</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.893896</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>1335.311579</td>\n",
       "      <td>355.351127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839712</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.260404</td>\n",
       "      <td>0.258348</td>\n",
       "      <td>9.661947</td>\n",
       "      <td>204.512448</td>\n",
       "      <td>148.496988</td>\n",
       "      <td>1.676433</td>\n",
       "      <td>2.844376</td>\n",
       "      <td>3.398501</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.680220</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.290018</td>\n",
       "      <td>1241.302609</td>\n",
       "      <td>15.320134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839713</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.546442</td>\n",
       "      <td>0.287672</td>\n",
       "      <td>5.356238</td>\n",
       "      <td>70.709555</td>\n",
       "      <td>273.483265</td>\n",
       "      <td>1.813901</td>\n",
       "      <td>3.278983</td>\n",
       "      <td>4.063580</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.815280</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.242551</td>\n",
       "      <td>1484.222588</td>\n",
       "      <td>20.432959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839714 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name         a         e          i          om           w  \\\n",
       "0         Ceres  2.769165  0.076009  10.594067   80.305532   73.597694   \n",
       "1        Pallas  2.772466  0.230337  34.836234  173.080063  310.048857   \n",
       "2          Juno  2.669150  0.256942  12.988919  169.852760  248.138626   \n",
       "3         Vesta  2.361418  0.088721   7.141771  103.810804  150.728541   \n",
       "4       Astraea  2.574249  0.191095   5.366988  141.576605  358.687607   \n",
       "...         ...       ...       ...        ...         ...         ...   \n",
       "839709      NaN  2.812945  0.664688   4.695700  183.310012  234.618352   \n",
       "839710      NaN  2.645238  0.259376  12.574937    1.620020  339.568072   \n",
       "839711      NaN  2.373137  0.202053   0.732484  176.499082  198.026527   \n",
       "839712      NaN  2.260404  0.258348   9.661947  204.512448  148.496988   \n",
       "839713      NaN  2.546442  0.287672   5.356238   70.709555  273.483265   \n",
       "\n",
       "               q        ad     per_y  data_arc  ...     UB  IR  spec_B spec_T  \\\n",
       "0       2.558684  2.979647  4.608202    8822.0  ...  0.426 NaN       C      G   \n",
       "1       2.133865  3.411067  4.616444   72318.0  ...  0.284 NaN       B      B   \n",
       "2       1.983332  3.354967  4.360814   72684.0  ...  0.433 NaN      Sk      S   \n",
       "3       2.151909  2.570926  3.628837   24288.0  ...  0.492 NaN       V      V   \n",
       "4       2.082324  3.066174  4.130323   63507.0  ...  0.411 NaN       S      S   \n",
       "...          ...       ...       ...       ...  ...    ...  ..     ...    ...   \n",
       "839709  0.943214  4.682676  4.717914   17298.0  ...    NaN NaN     NaN    NaN   \n",
       "839710  1.959126  3.331350  4.302346      16.0  ...    NaN NaN     NaN    NaN   \n",
       "839711  1.893638  2.852636  3.655884       5.0  ...    NaN NaN     NaN    NaN   \n",
       "839712  1.676433  2.844376  3.398501      10.0  ...    NaN NaN     NaN    NaN   \n",
       "839713  1.813901  3.278983  4.063580      11.0  ...    NaN NaN     NaN    NaN   \n",
       "\n",
       "           G      moid class         n          per          ma  \n",
       "0       0.12  1.594780   MBA  0.213885  1683.145708   77.372096  \n",
       "1       0.11  1.233240   MBA  0.213503  1686.155999   59.699133  \n",
       "2       0.32  1.034540   MBA  0.226019  1592.787285   34.925016  \n",
       "3       0.32  1.139480   MBA  0.271609  1325.432765   95.861936  \n",
       "4        NaN  1.095890   MBA  0.238632  1508.600458  282.366289  \n",
       "...      ...       ...   ...       ...          ...         ...  \n",
       "839709   NaN  0.032397   APO  0.208911  1723.217927  156.905910  \n",
       "839710   NaN  0.956145   MBA  0.229090  1571.431965   13.366251  \n",
       "839711   NaN  0.893896   MBA  0.269600  1335.311579  355.351127  \n",
       "839712   NaN  0.680220   MBA  0.290018  1241.302609   15.320134  \n",
       "839713   NaN  0.815280   MBA  0.242551  1484.222588   20.432959  \n",
       "\n",
       "[839714 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Asteroid_updated.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8e4dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'a', 'e', 'i', 'om', 'w', 'q', 'ad', 'per_y', 'data_arc',\n",
       "       'condition_code', 'n_obs_used', 'H', 'neo', 'pha', 'diameter', 'extent',\n",
       "       'albedo', 'rot_per', 'GM', 'BV', 'UB', 'IR', 'spec_B', 'spec_T', 'G',\n",
       "       'moid', 'class', 'n', 'per', 'ma'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff45b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(839714, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78883ff8",
   "metadata": {},
   "source": [
    "The diameter column includes some empty (NaN) values. We need to delete these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ca379f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15124, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['name', 'diameter'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f356df",
   "metadata": {},
   "source": [
    "Drop columns having empty values (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9b3943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15124, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee5721",
   "metadata": {},
   "source": [
    "We will delete the 'neo' (Near Earth Object), 'pha' (Potential Hazardous Asteroid) and 'class' columns as they contain non-numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c730c59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15124, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['neo', 'pha', 'class'], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150f297",
   "metadata": {},
   "source": [
    "## 2. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ce9eb",
   "metadata": {},
   "source": [
    "Set the Asteriod name as the index of the rows. Convert the values to float (in case there are some numbers representated as strings). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b548ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('name')\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a91cf5",
   "metadata": {},
   "source": [
    "Draw a head map of the correlation matrix to select some features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f0384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb34ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_907ad_row0_col0, #T_907ad_row1_col1, #T_907ad_row2_col2, #T_907ad_row3_col3, #T_907ad_row4_col4, #T_907ad_row5_col5, #T_907ad_row5_col13, #T_907ad_row6_col6, #T_907ad_row7_col7, #T_907ad_row7_col15, #T_907ad_row8_col8, #T_907ad_row9_col9, #T_907ad_row10_col10, #T_907ad_row11_col11, #T_907ad_row12_col12, #T_907ad_row13_col5, #T_907ad_row13_col13, #T_907ad_row14_col14, #T_907ad_row15_col7, #T_907ad_row15_col15, #T_907ad_row16_col16 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row0_col1, #T_907ad_row0_col11 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row0_col2 {\n",
       "  background-color: #9bbcff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row0_col3, #T_907ad_row5_col3, #T_907ad_row13_col3, #T_907ad_row14_col4 {\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row0_col4, #T_907ad_row6_col4 {\n",
       "  background-color: #6282ea;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row0_col5, #T_907ad_row0_col7, #T_907ad_row0_col13, #T_907ad_row0_col15, #T_907ad_row7_col6, #T_907ad_row13_col0, #T_907ad_row15_col6 {\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row0_col6, #T_907ad_row6_col0 {\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row0_col8, #T_907ad_row2_col0, #T_907ad_row2_col13 {\n",
       "  background-color: #cfdaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row0_col9, #T_907ad_row11_col14, #T_907ad_row12_col13 {\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row0_col10, #T_907ad_row4_col13, #T_907ad_row7_col10, #T_907ad_row8_col0, #T_907ad_row8_col13, #T_907ad_row9_col11, #T_907ad_row14_col8, #T_907ad_row15_col10 {\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row0_col12 {\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row0_col14 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row0_col16 {\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row1_col0, #T_907ad_row10_col0, #T_907ad_row16_col6 {\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row1_col2 {\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row1_col3, #T_907ad_row12_col4 {\n",
       "  background-color: #5f7fe8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row1_col4, #T_907ad_row2_col3, #T_907ad_row2_col4, #T_907ad_row9_col3, #T_907ad_row9_col4, #T_907ad_row12_col3 {\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row1_col5 {\n",
       "  background-color: #7093f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row1_col6, #T_907ad_row1_col11, #T_907ad_row2_col8 {\n",
       "  background-color: #d4dbe6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row1_col7, #T_907ad_row1_col15, #T_907ad_row6_col11, #T_907ad_row12_col1, #T_907ad_row16_col7, #T_907ad_row16_col15 {\n",
       "  background-color: #93b5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row1_col8 {\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row1_col9 {\n",
       "  background-color: #6687ed;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row1_col10, #T_907ad_row4_col5, #T_907ad_row6_col1, #T_907ad_row8_col5, #T_907ad_row10_col14, #T_907ad_row14_col1 {\n",
       "  background-color: #c1d4f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row1_col12, #T_907ad_row5_col10 {\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row1_col13, #T_907ad_row11_col0 {\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row1_col14 {\n",
       "  background-color: #dedcdb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row1_col16, #T_907ad_row9_col16 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row2_col1 {\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row2_col5, #T_907ad_row4_col10, #T_907ad_row5_col8, #T_907ad_row7_col8, #T_907ad_row15_col8, #T_907ad_row16_col8 {\n",
       "  background-color: #ccd9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row2_col6, #T_907ad_row3_col11, #T_907ad_row14_col10 {\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row2_col7, #T_907ad_row2_col15, #T_907ad_row11_col1 {\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row2_col9 {\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row2_col10, #T_907ad_row6_col10, #T_907ad_row9_col10, #T_907ad_row16_col0 {\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row2_col11 {\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row2_col12 {\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row2_col14, #T_907ad_row4_col6, #T_907ad_row8_col14, #T_907ad_row9_col14 {\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row2_col16, #T_907ad_row4_col16, #T_907ad_row6_col16 {\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row3_col0, #T_907ad_row8_col6 {\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col1, #T_907ad_row13_col2 {\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col2 {\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row3_col4, #T_907ad_row4_col3, #T_907ad_row5_col1, #T_907ad_row5_col14, #T_907ad_row8_col11, #T_907ad_row11_col2, #T_907ad_row11_col8, #T_907ad_row11_col9, #T_907ad_row11_col10, #T_907ad_row11_col12, #T_907ad_row14_col0, #T_907ad_row14_col5, #T_907ad_row14_col6, #T_907ad_row14_col7, #T_907ad_row14_col9, #T_907ad_row14_col13, #T_907ad_row14_col15, #T_907ad_row14_col16 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row3_col5, #T_907ad_row4_col14 {\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col6 {\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col7, #T_907ad_row3_col15 {\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col8 {\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col9, #T_907ad_row4_col9 {\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row3_col10, #T_907ad_row13_col10 {\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col12, #T_907ad_row3_col14, #T_907ad_row16_col10, #T_907ad_row16_col13 {\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col13, #T_907ad_row10_col5 {\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row3_col16 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row4_col0 {\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row4_col1, #T_907ad_row4_col7, #T_907ad_row4_col15, #T_907ad_row7_col1, #T_907ad_row15_col1 {\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row4_col2 {\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row4_col8, #T_907ad_row4_col12, #T_907ad_row16_col5, #T_907ad_row16_col11 {\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row4_col11 {\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row5_col0 {\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row5_col2 {\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row5_col4, #T_907ad_row7_col4, #T_907ad_row13_col4, #T_907ad_row15_col4, #T_907ad_row16_col4 {\n",
       "  background-color: #6180e9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row5_col6 {\n",
       "  background-color: #e46e56;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row5_col7, #T_907ad_row5_col15, #T_907ad_row6_col5, #T_907ad_row6_col13 {\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row5_col9 {\n",
       "  background-color: #d8dce2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row5_col11 {\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row5_col12, #T_907ad_row13_col12 {\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row5_col16, #T_907ad_row13_col16 {\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row6_col2 {\n",
       "  background-color: #9fbfff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row6_col3, #T_907ad_row11_col4, #T_907ad_row11_col7, #T_907ad_row11_col15 {\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row6_col7, #T_907ad_row6_col15 {\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row6_col8 {\n",
       "  background-color: #d1dae9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row6_col9 {\n",
       "  background-color: #f7b093;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row6_col12 {\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row6_col14 {\n",
       "  background-color: #516ddb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row7_col0, #T_907ad_row15_col0 {\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row7_col2, #T_907ad_row15_col2, #T_907ad_row16_col1 {\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row7_col3, #T_907ad_row15_col3, #T_907ad_row16_col3 {\n",
       "  background-color: #5d7ce6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row7_col5, #T_907ad_row7_col13, #T_907ad_row15_col5, #T_907ad_row15_col13 {\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row7_col9, #T_907ad_row15_col9 {\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row7_col11, #T_907ad_row15_col11 {\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row7_col12, #T_907ad_row12_col0, #T_907ad_row15_col12 {\n",
       "  background-color: #f5c2aa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row7_col14, #T_907ad_row11_col13, #T_907ad_row15_col14 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row7_col16, #T_907ad_row8_col9, #T_907ad_row15_col16 {\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row8_col1 {\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row8_col2 {\n",
       "  background-color: #8caffe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row8_col3, #T_907ad_row10_col4 {\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row8_col4 {\n",
       "  background-color: #5673e0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row8_col7, #T_907ad_row8_col15 {\n",
       "  background-color: #94b6ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row8_col10, #T_907ad_row10_col8 {\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row8_col12 {\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row8_col16, #T_907ad_row16_col9 {\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row9_col0 {\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row9_col1, #T_907ad_row12_col2 {\n",
       "  background-color: #abc8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row9_col2 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row9_col5, #T_907ad_row9_col13 {\n",
       "  background-color: #f7b79b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row9_col6 {\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row9_col7, #T_907ad_row9_col15 {\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row9_col8, #T_907ad_row16_col12 {\n",
       "  background-color: #c7d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row9_col12 {\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row10_col1 {\n",
       "  background-color: #8badfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row10_col2 {\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row10_col3 {\n",
       "  background-color: #5572df;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row10_col6 {\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row10_col7, #T_907ad_row10_col15, #T_907ad_row12_col9, #T_907ad_row13_col11 {\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row10_col9, #T_907ad_row11_col16 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row10_col11, #T_907ad_row10_col16, #T_907ad_row13_col1 {\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row10_col12 {\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row10_col13, #T_907ad_row16_col14 {\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row11_col3 {\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row11_col5 {\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row11_col6 {\n",
       "  background-color: #6b8df0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row12_col5 {\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row12_col6 {\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row12_col7, #T_907ad_row12_col15 {\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row12_col8 {\n",
       "  background-color: #f59d7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row12_col10, #T_907ad_row14_col11 {\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row12_col11 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row12_col14 {\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row12_col16 {\n",
       "  background-color: #4961d2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row13_col6 {\n",
       "  background-color: #e36c55;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row13_col7, #T_907ad_row13_col15 {\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row13_col8 {\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row13_col9 {\n",
       "  background-color: #d9dce1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row13_col14 {\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row14_col2 {\n",
       "  background-color: #688aef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row14_col3 {\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_907ad_row14_col12 {\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_907ad_row16_col2 {\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_907ad\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_907ad_level0_col0\" class=\"col_heading level0 col0\" >a</th>\n",
       "      <th id=\"T_907ad_level0_col1\" class=\"col_heading level0 col1\" >e</th>\n",
       "      <th id=\"T_907ad_level0_col2\" class=\"col_heading level0 col2\" >i</th>\n",
       "      <th id=\"T_907ad_level0_col3\" class=\"col_heading level0 col3\" >om</th>\n",
       "      <th id=\"T_907ad_level0_col4\" class=\"col_heading level0 col4\" >w</th>\n",
       "      <th id=\"T_907ad_level0_col5\" class=\"col_heading level0 col5\" >q</th>\n",
       "      <th id=\"T_907ad_level0_col6\" class=\"col_heading level0 col6\" >ad</th>\n",
       "      <th id=\"T_907ad_level0_col7\" class=\"col_heading level0 col7\" >per_y</th>\n",
       "      <th id=\"T_907ad_level0_col8\" class=\"col_heading level0 col8\" >data_arc</th>\n",
       "      <th id=\"T_907ad_level0_col9\" class=\"col_heading level0 col9\" >condition_code</th>\n",
       "      <th id=\"T_907ad_level0_col10\" class=\"col_heading level0 col10\" >n_obs_used</th>\n",
       "      <th id=\"T_907ad_level0_col11\" class=\"col_heading level0 col11\" >H</th>\n",
       "      <th id=\"T_907ad_level0_col12\" class=\"col_heading level0 col12\" >diameter</th>\n",
       "      <th id=\"T_907ad_level0_col13\" class=\"col_heading level0 col13\" >moid</th>\n",
       "      <th id=\"T_907ad_level0_col14\" class=\"col_heading level0 col14\" >n</th>\n",
       "      <th id=\"T_907ad_level0_col15\" class=\"col_heading level0 col15\" >per</th>\n",
       "      <th id=\"T_907ad_level0_col16\" class=\"col_heading level0 col16\" >ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row0\" class=\"row_heading level0 row0\" >a</th>\n",
       "      <td id=\"T_907ad_row0_col0\" class=\"data row0 col0\" >1.00</td>\n",
       "      <td id=\"T_907ad_row0_col1\" class=\"data row0 col1\" >-0.05</td>\n",
       "      <td id=\"T_907ad_row0_col2\" class=\"data row0 col2\" >0.11</td>\n",
       "      <td id=\"T_907ad_row0_col3\" class=\"data row0 col3\" >-0.01</td>\n",
       "      <td id=\"T_907ad_row0_col4\" class=\"data row0 col4\" >0.02</td>\n",
       "      <td id=\"T_907ad_row0_col5\" class=\"data row0 col5\" >0.91</td>\n",
       "      <td id=\"T_907ad_row0_col6\" class=\"data row0 col6\" >0.96</td>\n",
       "      <td id=\"T_907ad_row0_col7\" class=\"data row0 col7\" >0.93</td>\n",
       "      <td id=\"T_907ad_row0_col8\" class=\"data row0 col8\" >0.03</td>\n",
       "      <td id=\"T_907ad_row0_col9\" class=\"data row0 col9\" >0.62</td>\n",
       "      <td id=\"T_907ad_row0_col10\" class=\"data row0 col10\" >-0.04</td>\n",
       "      <td id=\"T_907ad_row0_col11\" class=\"data row0 col11\" >-0.34</td>\n",
       "      <td id=\"T_907ad_row0_col12\" class=\"data row0 col12\" >0.40</td>\n",
       "      <td id=\"T_907ad_row0_col13\" class=\"data row0 col13\" >0.91</td>\n",
       "      <td id=\"T_907ad_row0_col14\" class=\"data row0 col14\" >-0.62</td>\n",
       "      <td id=\"T_907ad_row0_col15\" class=\"data row0 col15\" >0.93</td>\n",
       "      <td id=\"T_907ad_row0_col16\" class=\"data row0 col16\" >0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row1\" class=\"row_heading level0 row1\" >e</th>\n",
       "      <td id=\"T_907ad_row1_col0\" class=\"data row1 col0\" >-0.05</td>\n",
       "      <td id=\"T_907ad_row1_col1\" class=\"data row1 col1\" >1.00</td>\n",
       "      <td id=\"T_907ad_row1_col2\" class=\"data row1 col2\" >0.15</td>\n",
       "      <td id=\"T_907ad_row1_col3\" class=\"data row1 col3\" >0.01</td>\n",
       "      <td id=\"T_907ad_row1_col4\" class=\"data row1 col4\" >0.00</td>\n",
       "      <td id=\"T_907ad_row1_col5\" class=\"data row1 col5\" >-0.38</td>\n",
       "      <td id=\"T_907ad_row1_col6\" class=\"data row1 col6\" >0.18</td>\n",
       "      <td id=\"T_907ad_row1_col7\" class=\"data row1 col7\" >0.00</td>\n",
       "      <td id=\"T_907ad_row1_col8\" class=\"data row1 col8\" >0.07</td>\n",
       "      <td id=\"T_907ad_row1_col9\" class=\"data row1 col9\" >0.09</td>\n",
       "      <td id=\"T_907ad_row1_col10\" class=\"data row1 col10\" >-0.04</td>\n",
       "      <td id=\"T_907ad_row1_col11\" class=\"data row1 col11\" >0.06</td>\n",
       "      <td id=\"T_907ad_row1_col12\" class=\"data row1 col12\" >-0.01</td>\n",
       "      <td id=\"T_907ad_row1_col13\" class=\"data row1 col13\" >-0.36</td>\n",
       "      <td id=\"T_907ad_row1_col14\" class=\"data row1 col14\" >0.18</td>\n",
       "      <td id=\"T_907ad_row1_col15\" class=\"data row1 col15\" >0.00</td>\n",
       "      <td id=\"T_907ad_row1_col16\" class=\"data row1 col16\" >-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row2\" class=\"row_heading level0 row2\" >i</th>\n",
       "      <td id=\"T_907ad_row2_col0\" class=\"data row2 col0\" >0.11</td>\n",
       "      <td id=\"T_907ad_row2_col1\" class=\"data row2 col1\" >0.15</td>\n",
       "      <td id=\"T_907ad_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
       "      <td id=\"T_907ad_row2_col3\" class=\"data row2 col3\" >0.00</td>\n",
       "      <td id=\"T_907ad_row2_col4\" class=\"data row2 col4\" >0.00</td>\n",
       "      <td id=\"T_907ad_row2_col5\" class=\"data row2 col5\" >0.07</td>\n",
       "      <td id=\"T_907ad_row2_col6\" class=\"data row2 col6\" >0.13</td>\n",
       "      <td id=\"T_907ad_row2_col7\" class=\"data row2 col7\" >0.07</td>\n",
       "      <td id=\"T_907ad_row2_col8\" class=\"data row2 col8\" >0.06</td>\n",
       "      <td id=\"T_907ad_row2_col9\" class=\"data row2 col9\" >0.03</td>\n",
       "      <td id=\"T_907ad_row2_col10\" class=\"data row2 col10\" >-0.05</td>\n",
       "      <td id=\"T_907ad_row2_col11\" class=\"data row2 col11\" >-0.25</td>\n",
       "      <td id=\"T_907ad_row2_col12\" class=\"data row2 col12\" >0.18</td>\n",
       "      <td id=\"T_907ad_row2_col13\" class=\"data row2 col13\" >0.10</td>\n",
       "      <td id=\"T_907ad_row2_col14\" class=\"data row2 col14\" >-0.06</td>\n",
       "      <td id=\"T_907ad_row2_col15\" class=\"data row2 col15\" >0.07</td>\n",
       "      <td id=\"T_907ad_row2_col16\" class=\"data row2 col16\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row3\" class=\"row_heading level0 row3\" >om</th>\n",
       "      <td id=\"T_907ad_row3_col0\" class=\"data row3 col0\" >-0.01</td>\n",
       "      <td id=\"T_907ad_row3_col1\" class=\"data row3 col1\" >0.01</td>\n",
       "      <td id=\"T_907ad_row3_col2\" class=\"data row3 col2\" >0.00</td>\n",
       "      <td id=\"T_907ad_row3_col3\" class=\"data row3 col3\" >1.00</td>\n",
       "      <td id=\"T_907ad_row3_col4\" class=\"data row3 col4\" >-0.13</td>\n",
       "      <td id=\"T_907ad_row3_col5\" class=\"data row3 col5\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row3_col6\" class=\"data row3 col6\" >-0.01</td>\n",
       "      <td id=\"T_907ad_row3_col7\" class=\"data row3 col7\" >-0.00</td>\n",
       "      <td id=\"T_907ad_row3_col8\" class=\"data row3 col8\" >0.03</td>\n",
       "      <td id=\"T_907ad_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
       "      <td id=\"T_907ad_row3_col10\" class=\"data row3 col10\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row3_col11\" class=\"data row3 col11\" >-0.00</td>\n",
       "      <td id=\"T_907ad_row3_col12\" class=\"data row3 col12\" >0.00</td>\n",
       "      <td id=\"T_907ad_row3_col13\" class=\"data row3 col13\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row3_col14\" class=\"data row3 col14\" >0.03</td>\n",
       "      <td id=\"T_907ad_row3_col15\" class=\"data row3 col15\" >-0.00</td>\n",
       "      <td id=\"T_907ad_row3_col16\" class=\"data row3 col16\" >-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row4\" class=\"row_heading level0 row4\" >w</th>\n",
       "      <td id=\"T_907ad_row4_col0\" class=\"data row4 col0\" >0.02</td>\n",
       "      <td id=\"T_907ad_row4_col1\" class=\"data row4 col1\" >0.00</td>\n",
       "      <td id=\"T_907ad_row4_col2\" class=\"data row4 col2\" >0.00</td>\n",
       "      <td id=\"T_907ad_row4_col3\" class=\"data row4 col3\" >-0.13</td>\n",
       "      <td id=\"T_907ad_row4_col4\" class=\"data row4 col4\" >1.00</td>\n",
       "      <td id=\"T_907ad_row4_col5\" class=\"data row4 col5\" >0.01</td>\n",
       "      <td id=\"T_907ad_row4_col6\" class=\"data row4 col6\" >0.02</td>\n",
       "      <td id=\"T_907ad_row4_col7\" class=\"data row4 col7\" >0.01</td>\n",
       "      <td id=\"T_907ad_row4_col8\" class=\"data row4 col8\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row4_col9\" class=\"data row4 col9\" >0.00</td>\n",
       "      <td id=\"T_907ad_row4_col10\" class=\"data row4 col10\" >0.02</td>\n",
       "      <td id=\"T_907ad_row4_col11\" class=\"data row4 col11\" >-0.01</td>\n",
       "      <td id=\"T_907ad_row4_col12\" class=\"data row4 col12\" >0.01</td>\n",
       "      <td id=\"T_907ad_row4_col13\" class=\"data row4 col13\" >0.01</td>\n",
       "      <td id=\"T_907ad_row4_col14\" class=\"data row4 col14\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row4_col15\" class=\"data row4 col15\" >0.01</td>\n",
       "      <td id=\"T_907ad_row4_col16\" class=\"data row4 col16\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row5\" class=\"row_heading level0 row5\" >q</th>\n",
       "      <td id=\"T_907ad_row5_col0\" class=\"data row5 col0\" >0.91</td>\n",
       "      <td id=\"T_907ad_row5_col1\" class=\"data row5 col1\" >-0.38</td>\n",
       "      <td id=\"T_907ad_row5_col2\" class=\"data row5 col2\" >0.07</td>\n",
       "      <td id=\"T_907ad_row5_col3\" class=\"data row5 col3\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row5_col4\" class=\"data row5 col4\" >0.01</td>\n",
       "      <td id=\"T_907ad_row5_col5\" class=\"data row5 col5\" >1.00</td>\n",
       "      <td id=\"T_907ad_row5_col6\" class=\"data row5 col6\" >0.76</td>\n",
       "      <td id=\"T_907ad_row5_col7\" class=\"data row5 col7\" >0.81</td>\n",
       "      <td id=\"T_907ad_row5_col8\" class=\"data row5 col8\" >0.01</td>\n",
       "      <td id=\"T_907ad_row5_col9\" class=\"data row5 col9\" >0.45</td>\n",
       "      <td id=\"T_907ad_row5_col10\" class=\"data row5 col10\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row5_col11\" class=\"data row5 col11\" >-0.35</td>\n",
       "      <td id=\"T_907ad_row5_col12\" class=\"data row5 col12\" >0.40</td>\n",
       "      <td id=\"T_907ad_row5_col13\" class=\"data row5 col13\" >1.00</td>\n",
       "      <td id=\"T_907ad_row5_col14\" class=\"data row5 col14\" >-0.66</td>\n",
       "      <td id=\"T_907ad_row5_col15\" class=\"data row5 col15\" >0.81</td>\n",
       "      <td id=\"T_907ad_row5_col16\" class=\"data row5 col16\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row6\" class=\"row_heading level0 row6\" >ad</th>\n",
       "      <td id=\"T_907ad_row6_col0\" class=\"data row6 col0\" >0.96</td>\n",
       "      <td id=\"T_907ad_row6_col1\" class=\"data row6 col1\" >0.18</td>\n",
       "      <td id=\"T_907ad_row6_col2\" class=\"data row6 col2\" >0.13</td>\n",
       "      <td id=\"T_907ad_row6_col3\" class=\"data row6 col3\" >-0.01</td>\n",
       "      <td id=\"T_907ad_row6_col4\" class=\"data row6 col4\" >0.02</td>\n",
       "      <td id=\"T_907ad_row6_col5\" class=\"data row6 col5\" >0.76</td>\n",
       "      <td id=\"T_907ad_row6_col6\" class=\"data row6 col6\" >1.00</td>\n",
       "      <td id=\"T_907ad_row6_col7\" class=\"data row6 col7\" >0.92</td>\n",
       "      <td id=\"T_907ad_row6_col8\" class=\"data row6 col8\" >0.04</td>\n",
       "      <td id=\"T_907ad_row6_col9\" class=\"data row6 col9\" >0.67</td>\n",
       "      <td id=\"T_907ad_row6_col10\" class=\"data row6 col10\" >-0.05</td>\n",
       "      <td id=\"T_907ad_row6_col11\" class=\"data row6 col11\" >-0.30</td>\n",
       "      <td id=\"T_907ad_row6_col12\" class=\"data row6 col12\" >0.36</td>\n",
       "      <td id=\"T_907ad_row6_col13\" class=\"data row6 col13\" >0.77</td>\n",
       "      <td id=\"T_907ad_row6_col14\" class=\"data row6 col14\" >-0.53</td>\n",
       "      <td id=\"T_907ad_row6_col15\" class=\"data row6 col15\" >0.92</td>\n",
       "      <td id=\"T_907ad_row6_col16\" class=\"data row6 col16\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row7\" class=\"row_heading level0 row7\" >per_y</th>\n",
       "      <td id=\"T_907ad_row7_col0\" class=\"data row7 col0\" >0.93</td>\n",
       "      <td id=\"T_907ad_row7_col1\" class=\"data row7 col1\" >0.00</td>\n",
       "      <td id=\"T_907ad_row7_col2\" class=\"data row7 col2\" >0.07</td>\n",
       "      <td id=\"T_907ad_row7_col3\" class=\"data row7 col3\" >-0.00</td>\n",
       "      <td id=\"T_907ad_row7_col4\" class=\"data row7 col4\" >0.01</td>\n",
       "      <td id=\"T_907ad_row7_col5\" class=\"data row7 col5\" >0.81</td>\n",
       "      <td id=\"T_907ad_row7_col6\" class=\"data row7 col6\" >0.92</td>\n",
       "      <td id=\"T_907ad_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "      <td id=\"T_907ad_row7_col8\" class=\"data row7 col8\" >0.01</td>\n",
       "      <td id=\"T_907ad_row7_col9\" class=\"data row7 col9\" >0.73</td>\n",
       "      <td id=\"T_907ad_row7_col10\" class=\"data row7 col10\" >-0.04</td>\n",
       "      <td id=\"T_907ad_row7_col11\" class=\"data row7 col11\" >-0.22</td>\n",
       "      <td id=\"T_907ad_row7_col12\" class=\"data row7 col12\" >0.37</td>\n",
       "      <td id=\"T_907ad_row7_col13\" class=\"data row7 col13\" >0.81</td>\n",
       "      <td id=\"T_907ad_row7_col14\" class=\"data row7 col14\" >-0.36</td>\n",
       "      <td id=\"T_907ad_row7_col15\" class=\"data row7 col15\" >1.00</td>\n",
       "      <td id=\"T_907ad_row7_col16\" class=\"data row7 col16\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row8\" class=\"row_heading level0 row8\" >data_arc</th>\n",
       "      <td id=\"T_907ad_row8_col0\" class=\"data row8 col0\" >0.03</td>\n",
       "      <td id=\"T_907ad_row8_col1\" class=\"data row8 col1\" >0.07</td>\n",
       "      <td id=\"T_907ad_row8_col2\" class=\"data row8 col2\" >0.06</td>\n",
       "      <td id=\"T_907ad_row8_col3\" class=\"data row8 col3\" >0.03</td>\n",
       "      <td id=\"T_907ad_row8_col4\" class=\"data row8 col4\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row8_col5\" class=\"data row8 col5\" >0.01</td>\n",
       "      <td id=\"T_907ad_row8_col6\" class=\"data row8 col6\" >0.04</td>\n",
       "      <td id=\"T_907ad_row8_col7\" class=\"data row8 col7\" >0.01</td>\n",
       "      <td id=\"T_907ad_row8_col8\" class=\"data row8 col8\" >1.00</td>\n",
       "      <td id=\"T_907ad_row8_col9\" class=\"data row8 col9\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row8_col10\" class=\"data row8 col10\" >0.71</td>\n",
       "      <td id=\"T_907ad_row8_col11\" class=\"data row8 col11\" >-0.77</td>\n",
       "      <td id=\"T_907ad_row8_col12\" class=\"data row8 col12\" >0.54</td>\n",
       "      <td id=\"T_907ad_row8_col13\" class=\"data row8 col13\" >0.01</td>\n",
       "      <td id=\"T_907ad_row8_col14\" class=\"data row8 col14\" >-0.06</td>\n",
       "      <td id=\"T_907ad_row8_col15\" class=\"data row8 col15\" >0.01</td>\n",
       "      <td id=\"T_907ad_row8_col16\" class=\"data row8 col16\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row9\" class=\"row_heading level0 row9\" >condition_code</th>\n",
       "      <td id=\"T_907ad_row9_col0\" class=\"data row9 col0\" >0.62</td>\n",
       "      <td id=\"T_907ad_row9_col1\" class=\"data row9 col1\" >0.09</td>\n",
       "      <td id=\"T_907ad_row9_col2\" class=\"data row9 col2\" >0.03</td>\n",
       "      <td id=\"T_907ad_row9_col3\" class=\"data row9 col3\" >0.00</td>\n",
       "      <td id=\"T_907ad_row9_col4\" class=\"data row9 col4\" >0.00</td>\n",
       "      <td id=\"T_907ad_row9_col5\" class=\"data row9 col5\" >0.45</td>\n",
       "      <td id=\"T_907ad_row9_col6\" class=\"data row9 col6\" >0.67</td>\n",
       "      <td id=\"T_907ad_row9_col7\" class=\"data row9 col7\" >0.73</td>\n",
       "      <td id=\"T_907ad_row9_col8\" class=\"data row9 col8\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row9_col9\" class=\"data row9 col9\" >1.00</td>\n",
       "      <td id=\"T_907ad_row9_col10\" class=\"data row9 col10\" >-0.05</td>\n",
       "      <td id=\"T_907ad_row9_col11\" class=\"data row9 col11\" >-0.06</td>\n",
       "      <td id=\"T_907ad_row9_col12\" class=\"data row9 col12\" >0.18</td>\n",
       "      <td id=\"T_907ad_row9_col13\" class=\"data row9 col13\" >0.46</td>\n",
       "      <td id=\"T_907ad_row9_col14\" class=\"data row9 col14\" >-0.06</td>\n",
       "      <td id=\"T_907ad_row9_col15\" class=\"data row9 col15\" >0.73</td>\n",
       "      <td id=\"T_907ad_row9_col16\" class=\"data row9 col16\" >-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row10\" class=\"row_heading level0 row10\" >n_obs_used</th>\n",
       "      <td id=\"T_907ad_row10_col0\" class=\"data row10 col0\" >-0.04</td>\n",
       "      <td id=\"T_907ad_row10_col1\" class=\"data row10 col1\" >-0.04</td>\n",
       "      <td id=\"T_907ad_row10_col2\" class=\"data row10 col2\" >-0.05</td>\n",
       "      <td id=\"T_907ad_row10_col3\" class=\"data row10 col3\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row10_col4\" class=\"data row10 col4\" >0.02</td>\n",
       "      <td id=\"T_907ad_row10_col5\" class=\"data row10 col5\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row10_col6\" class=\"data row10 col6\" >-0.05</td>\n",
       "      <td id=\"T_907ad_row10_col7\" class=\"data row10 col7\" >-0.04</td>\n",
       "      <td id=\"T_907ad_row10_col8\" class=\"data row10 col8\" >0.71</td>\n",
       "      <td id=\"T_907ad_row10_col9\" class=\"data row10 col9\" >-0.05</td>\n",
       "      <td id=\"T_907ad_row10_col10\" class=\"data row10 col10\" >1.00</td>\n",
       "      <td id=\"T_907ad_row10_col11\" class=\"data row10 col11\" >-0.74</td>\n",
       "      <td id=\"T_907ad_row10_col12\" class=\"data row10 col12\" >0.41</td>\n",
       "      <td id=\"T_907ad_row10_col13\" class=\"data row10 col13\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row10_col14\" class=\"data row10 col14\" >0.01</td>\n",
       "      <td id=\"T_907ad_row10_col15\" class=\"data row10 col15\" >-0.04</td>\n",
       "      <td id=\"T_907ad_row10_col16\" class=\"data row10 col16\" >-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row11\" class=\"row_heading level0 row11\" >H</th>\n",
       "      <td id=\"T_907ad_row11_col0\" class=\"data row11 col0\" >-0.34</td>\n",
       "      <td id=\"T_907ad_row11_col1\" class=\"data row11 col1\" >0.06</td>\n",
       "      <td id=\"T_907ad_row11_col2\" class=\"data row11 col2\" >-0.25</td>\n",
       "      <td id=\"T_907ad_row11_col3\" class=\"data row11 col3\" >-0.00</td>\n",
       "      <td id=\"T_907ad_row11_col4\" class=\"data row11 col4\" >-0.01</td>\n",
       "      <td id=\"T_907ad_row11_col5\" class=\"data row11 col5\" >-0.35</td>\n",
       "      <td id=\"T_907ad_row11_col6\" class=\"data row11 col6\" >-0.30</td>\n",
       "      <td id=\"T_907ad_row11_col7\" class=\"data row11 col7\" >-0.22</td>\n",
       "      <td id=\"T_907ad_row11_col8\" class=\"data row11 col8\" >-0.77</td>\n",
       "      <td id=\"T_907ad_row11_col9\" class=\"data row11 col9\" >-0.06</td>\n",
       "      <td id=\"T_907ad_row11_col10\" class=\"data row11 col10\" >-0.74</td>\n",
       "      <td id=\"T_907ad_row11_col11\" class=\"data row11 col11\" >1.00</td>\n",
       "      <td id=\"T_907ad_row11_col12\" class=\"data row11 col12\" >-0.72</td>\n",
       "      <td id=\"T_907ad_row11_col13\" class=\"data row11 col13\" >-0.36</td>\n",
       "      <td id=\"T_907ad_row11_col14\" class=\"data row11 col14\" >0.41</td>\n",
       "      <td id=\"T_907ad_row11_col15\" class=\"data row11 col15\" >-0.22</td>\n",
       "      <td id=\"T_907ad_row11_col16\" class=\"data row11 col16\" >-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row12\" class=\"row_heading level0 row12\" >diameter</th>\n",
       "      <td id=\"T_907ad_row12_col0\" class=\"data row12 col0\" >0.40</td>\n",
       "      <td id=\"T_907ad_row12_col1\" class=\"data row12 col1\" >-0.01</td>\n",
       "      <td id=\"T_907ad_row12_col2\" class=\"data row12 col2\" >0.18</td>\n",
       "      <td id=\"T_907ad_row12_col3\" class=\"data row12 col3\" >0.00</td>\n",
       "      <td id=\"T_907ad_row12_col4\" class=\"data row12 col4\" >0.01</td>\n",
       "      <td id=\"T_907ad_row12_col5\" class=\"data row12 col5\" >0.40</td>\n",
       "      <td id=\"T_907ad_row12_col6\" class=\"data row12 col6\" >0.36</td>\n",
       "      <td id=\"T_907ad_row12_col7\" class=\"data row12 col7\" >0.37</td>\n",
       "      <td id=\"T_907ad_row12_col8\" class=\"data row12 col8\" >0.54</td>\n",
       "      <td id=\"T_907ad_row12_col9\" class=\"data row12 col9\" >0.18</td>\n",
       "      <td id=\"T_907ad_row12_col10\" class=\"data row12 col10\" >0.41</td>\n",
       "      <td id=\"T_907ad_row12_col11\" class=\"data row12 col11\" >-0.72</td>\n",
       "      <td id=\"T_907ad_row12_col12\" class=\"data row12 col12\" >1.00</td>\n",
       "      <td id=\"T_907ad_row12_col13\" class=\"data row12 col13\" >0.41</td>\n",
       "      <td id=\"T_907ad_row12_col14\" class=\"data row12 col14\" >-0.28</td>\n",
       "      <td id=\"T_907ad_row12_col15\" class=\"data row12 col15\" >0.37</td>\n",
       "      <td id=\"T_907ad_row12_col16\" class=\"data row12 col16\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row13\" class=\"row_heading level0 row13\" >moid</th>\n",
       "      <td id=\"T_907ad_row13_col0\" class=\"data row13 col0\" >0.91</td>\n",
       "      <td id=\"T_907ad_row13_col1\" class=\"data row13 col1\" >-0.36</td>\n",
       "      <td id=\"T_907ad_row13_col2\" class=\"data row13 col2\" >0.10</td>\n",
       "      <td id=\"T_907ad_row13_col3\" class=\"data row13 col3\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row13_col4\" class=\"data row13 col4\" >0.01</td>\n",
       "      <td id=\"T_907ad_row13_col5\" class=\"data row13 col5\" >1.00</td>\n",
       "      <td id=\"T_907ad_row13_col6\" class=\"data row13 col6\" >0.77</td>\n",
       "      <td id=\"T_907ad_row13_col7\" class=\"data row13 col7\" >0.81</td>\n",
       "      <td id=\"T_907ad_row13_col8\" class=\"data row13 col8\" >0.01</td>\n",
       "      <td id=\"T_907ad_row13_col9\" class=\"data row13 col9\" >0.46</td>\n",
       "      <td id=\"T_907ad_row13_col10\" class=\"data row13 col10\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row13_col11\" class=\"data row13 col11\" >-0.36</td>\n",
       "      <td id=\"T_907ad_row13_col12\" class=\"data row13 col12\" >0.41</td>\n",
       "      <td id=\"T_907ad_row13_col13\" class=\"data row13 col13\" >1.00</td>\n",
       "      <td id=\"T_907ad_row13_col14\" class=\"data row13 col14\" >-0.65</td>\n",
       "      <td id=\"T_907ad_row13_col15\" class=\"data row13 col15\" >0.81</td>\n",
       "      <td id=\"T_907ad_row13_col16\" class=\"data row13 col16\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row14\" class=\"row_heading level0 row14\" >n</th>\n",
       "      <td id=\"T_907ad_row14_col0\" class=\"data row14 col0\" >-0.62</td>\n",
       "      <td id=\"T_907ad_row14_col1\" class=\"data row14 col1\" >0.18</td>\n",
       "      <td id=\"T_907ad_row14_col2\" class=\"data row14 col2\" >-0.06</td>\n",
       "      <td id=\"T_907ad_row14_col3\" class=\"data row14 col3\" >0.03</td>\n",
       "      <td id=\"T_907ad_row14_col4\" class=\"data row14 col4\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row14_col5\" class=\"data row14 col5\" >-0.66</td>\n",
       "      <td id=\"T_907ad_row14_col6\" class=\"data row14 col6\" >-0.53</td>\n",
       "      <td id=\"T_907ad_row14_col7\" class=\"data row14 col7\" >-0.36</td>\n",
       "      <td id=\"T_907ad_row14_col8\" class=\"data row14 col8\" >-0.06</td>\n",
       "      <td id=\"T_907ad_row14_col9\" class=\"data row14 col9\" >-0.06</td>\n",
       "      <td id=\"T_907ad_row14_col10\" class=\"data row14 col10\" >0.01</td>\n",
       "      <td id=\"T_907ad_row14_col11\" class=\"data row14 col11\" >0.41</td>\n",
       "      <td id=\"T_907ad_row14_col12\" class=\"data row14 col12\" >-0.28</td>\n",
       "      <td id=\"T_907ad_row14_col13\" class=\"data row14 col13\" >-0.65</td>\n",
       "      <td id=\"T_907ad_row14_col14\" class=\"data row14 col14\" >1.00</td>\n",
       "      <td id=\"T_907ad_row14_col15\" class=\"data row14 col15\" >-0.36</td>\n",
       "      <td id=\"T_907ad_row14_col16\" class=\"data row14 col16\" >-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row15\" class=\"row_heading level0 row15\" >per</th>\n",
       "      <td id=\"T_907ad_row15_col0\" class=\"data row15 col0\" >0.93</td>\n",
       "      <td id=\"T_907ad_row15_col1\" class=\"data row15 col1\" >0.00</td>\n",
       "      <td id=\"T_907ad_row15_col2\" class=\"data row15 col2\" >0.07</td>\n",
       "      <td id=\"T_907ad_row15_col3\" class=\"data row15 col3\" >-0.00</td>\n",
       "      <td id=\"T_907ad_row15_col4\" class=\"data row15 col4\" >0.01</td>\n",
       "      <td id=\"T_907ad_row15_col5\" class=\"data row15 col5\" >0.81</td>\n",
       "      <td id=\"T_907ad_row15_col6\" class=\"data row15 col6\" >0.92</td>\n",
       "      <td id=\"T_907ad_row15_col7\" class=\"data row15 col7\" >1.00</td>\n",
       "      <td id=\"T_907ad_row15_col8\" class=\"data row15 col8\" >0.01</td>\n",
       "      <td id=\"T_907ad_row15_col9\" class=\"data row15 col9\" >0.73</td>\n",
       "      <td id=\"T_907ad_row15_col10\" class=\"data row15 col10\" >-0.04</td>\n",
       "      <td id=\"T_907ad_row15_col11\" class=\"data row15 col11\" >-0.22</td>\n",
       "      <td id=\"T_907ad_row15_col12\" class=\"data row15 col12\" >0.37</td>\n",
       "      <td id=\"T_907ad_row15_col13\" class=\"data row15 col13\" >0.81</td>\n",
       "      <td id=\"T_907ad_row15_col14\" class=\"data row15 col14\" >-0.36</td>\n",
       "      <td id=\"T_907ad_row15_col15\" class=\"data row15 col15\" >1.00</td>\n",
       "      <td id=\"T_907ad_row15_col16\" class=\"data row15 col16\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_907ad_level0_row16\" class=\"row_heading level0 row16\" >ma</th>\n",
       "      <td id=\"T_907ad_row16_col0\" class=\"data row16 col0\" >0.02</td>\n",
       "      <td id=\"T_907ad_row16_col1\" class=\"data row16 col1\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row16_col2\" class=\"data row16 col2\" >0.01</td>\n",
       "      <td id=\"T_907ad_row16_col3\" class=\"data row16 col3\" >-0.00</td>\n",
       "      <td id=\"T_907ad_row16_col4\" class=\"data row16 col4\" >0.01</td>\n",
       "      <td id=\"T_907ad_row16_col5\" class=\"data row16 col5\" >0.04</td>\n",
       "      <td id=\"T_907ad_row16_col6\" class=\"data row16 col6\" >0.01</td>\n",
       "      <td id=\"T_907ad_row16_col7\" class=\"data row16 col7\" >0.00</td>\n",
       "      <td id=\"T_907ad_row16_col8\" class=\"data row16 col8\" >0.01</td>\n",
       "      <td id=\"T_907ad_row16_col9\" class=\"data row16 col9\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row16_col10\" class=\"data row16 col10\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row16_col11\" class=\"data row16 col11\" >-0.02</td>\n",
       "      <td id=\"T_907ad_row16_col12\" class=\"data row16 col12\" >0.01</td>\n",
       "      <td id=\"T_907ad_row16_col13\" class=\"data row16 col13\" >0.04</td>\n",
       "      <td id=\"T_907ad_row16_col14\" class=\"data row16 col14\" >-0.03</td>\n",
       "      <td id=\"T_907ad_row16_col15\" class=\"data row16 col15\" >0.00</td>\n",
       "      <td id=\"T_907ad_row16_col16\" class=\"data row16 col16\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f307b5acd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').format(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040932b",
   "metadata": {},
   "source": [
    "We will select a, q, ad, per_y, data_arc, n_obs_used, moid and per as the features to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d383ba",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bf775",
   "metadata": {},
   "source": [
    "We will use **Pytorch** package to train our ANN. ANN package normally have a data structure called **Tensors**. These can be thought of as multidimensional arrays with performance optimisation. They support mathemtical operations, like gradient computation, that are necessary for training ANN. \n",
    "<br>\n",
    "<br>\n",
    "We will first install **Pytorch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5388dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mohammed\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485988ed",
   "metadata": {},
   "source": [
    "We will import the ANN module and the optimisation module of **Pytorch** as well as **Pytorch** itself. We will also import numpy to store the Dataframe data into a numpy array and then move it to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a52e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6e5c8",
   "metadata": {},
   "source": [
    "Select the highest 8 correlated features and the diameter as the target value we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e36b6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.76916515e+00, 2.55868360e+00, 2.97964671e+00, ...,\n",
       "         1.00200000e+03, 1.59478000e+00, 1.68314571e+03],\n",
       "        [2.77246592e+00, 2.13386493e+00, 3.41106691e+00, ...,\n",
       "         8.49000000e+03, 1.23324000e+00, 1.68615600e+03],\n",
       "        [2.66914952e+00, 1.98333205e+00, 3.35496699e+00, ...,\n",
       "         7.10400000e+03, 1.03454000e+00, 1.59278729e+03],\n",
       "        ...,\n",
       "        [2.52500672e+00, 2.35410186e+00, 2.69591158e+00, ...,\n",
       "         9.50000000e+01, 1.33716000e+00, 1.46552163e+03],\n",
       "        [3.10269235e+00, 2.34337182e+00, 3.86201289e+00, ...,\n",
       "         8.20000000e+01, 1.36204000e+00, 1.99621125e+03],\n",
       "        [3.10748415e+00, 2.57470600e+00, 3.64026230e+00, ...,\n",
       "         1.05000000e+02, 1.72469000e+00, 2.00083746e+03]]),\n",
       " array([[939.4  ],\n",
       "        [545.   ],\n",
       "        [246.596],\n",
       "        ...,\n",
       "        [  2.155],\n",
       "        [  3.609],\n",
       "        [  3.655]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_np = df[['a', 'q', 'ad', 'per_y', 'data_arc', 'n_obs_used', 'moid', 'per']].values\n",
    "target_np = df[['diameter']].values\n",
    "features_np, target_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d43f7",
   "metadata": {},
   "source": [
    "Split the data into 70 percent training and 30 percent testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c880f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10586, 8), (4538, 8), (10586, 1), (4538, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_portion = int(features_np.shape[0] * 0.70)\n",
    "train_features_np = features_np[0:train_portion, :]\n",
    "test_features_np = features_np[train_portion:, :]\n",
    "train_target_np = target_np[0: train_portion, :]\n",
    "test_target_np = target_np[train_portion:, :]\n",
    "#\n",
    "train_features_np.shape, test_features_np.shape, train_target_np.shape, test_target_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659f651",
   "metadata": {},
   "source": [
    "Create tensors from Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b623e622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10586, 8]),\n",
       " torch.Size([4538, 8]),\n",
       " torch.Size([10586, 1]),\n",
       " torch.Size([4538, 1]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_tn = torch.tensor(train_features_np)\n",
    "test_features_tn = torch.tensor(test_features_np)\n",
    "train_target_tn = torch.tensor(train_target_np)\n",
    "test_target_tn = torch.tensor(test_target_np)\n",
    "#\n",
    "train_features_tn.shape, test_features_tn.shape, train_target_tn.shape, test_target_tn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf54f9b",
   "metadata": {},
   "source": [
    "Define the ANN artichecture as follows:\n",
    "<br>\n",
    ">1. 8 inputs.\n",
    ">2. 16 neurons in the hidden layer with Leaky ReLU activation functions.\n",
    ">3. 1 linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d7cb82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_arch = nn.Sequential(nn.Linear(8, 16, dtype=torch.float64), nn.LeakyReLU(), nn.Linear(16, 1, dtype=torch.float64))\n",
    "nn_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf7a272",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33370eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 502.6311\n",
      "Epoch 10, Training loss 1701.5556\n",
      "Epoch 20, Training loss 533.8069\n",
      "Epoch 30, Training loss 563.2246\n",
      "Epoch 40, Training loss 560.8805\n",
      "Epoch 50, Training loss 526.4258\n",
      "Epoch 60, Training loss 511.3374\n",
      "Epoch 70, Training loss 505.5851\n",
      "Epoch 80, Training loss 503.1863\n",
      "Epoch 90, Training loss 502.5813\n",
      "Epoch 100, Training loss 502.6621\n",
      "Epoch 110, Training loss 502.5913\n",
      "Epoch 120, Training loss 502.5376\n",
      "Epoch 130, Training loss 502.5263\n",
      "Epoch 140, Training loss 502.5038\n",
      "Epoch 150, Training loss 502.4868\n",
      "Epoch 160, Training loss 502.4685\n",
      "Epoch 170, Training loss 502.4499\n",
      "Epoch 180, Training loss 502.4312\n",
      "Epoch 190, Training loss 502.4118\n",
      "Epoch 200, Training loss 502.3913\n",
      "Epoch 210, Training loss 502.3709\n",
      "Epoch 220, Training loss 502.3498\n",
      "Epoch 230, Training loss 502.3280\n",
      "Epoch 240, Training loss 502.3062\n",
      "Epoch 250, Training loss 502.2830\n",
      "Epoch 260, Training loss 502.2595\n",
      "Epoch 270, Training loss 502.2353\n",
      "Epoch 280, Training loss 502.2112\n",
      "Epoch 290, Training loss 502.1863\n",
      "Epoch 300, Training loss 502.1612\n",
      "Epoch 310, Training loss 502.1353\n",
      "Epoch 320, Training loss 502.1091\n",
      "Epoch 330, Training loss 502.0824\n",
      "Epoch 340, Training loss 502.0552\n",
      "Epoch 350, Training loss 502.0277\n",
      "Epoch 360, Training loss 501.9997\n",
      "Epoch 370, Training loss 501.9710\n",
      "Epoch 380, Training loss 501.9418\n",
      "Epoch 390, Training loss 501.9127\n",
      "Epoch 400, Training loss 501.8830\n",
      "Epoch 410, Training loss 501.8524\n",
      "Epoch 420, Training loss 501.8224\n",
      "Epoch 430, Training loss 501.7913\n",
      "Epoch 440, Training loss 501.7598\n",
      "Epoch 450, Training loss 501.7276\n",
      "Epoch 460, Training loss 501.6951\n",
      "Epoch 470, Training loss 501.6623\n",
      "Epoch 480, Training loss 501.6294\n",
      "Epoch 490, Training loss 501.5955\n",
      "Epoch 500, Training loss 501.5615\n",
      "Epoch 510, Training loss 501.5273\n",
      "Epoch 520, Training loss 501.4934\n",
      "Epoch 530, Training loss 501.4581\n",
      "Epoch 540, Training loss 501.4226\n",
      "Epoch 550, Training loss 501.3863\n",
      "Epoch 560, Training loss 501.3508\n",
      "Epoch 570, Training loss 501.3134\n",
      "Epoch 580, Training loss 501.2762\n",
      "Epoch 590, Training loss 501.2390\n",
      "Epoch 600, Training loss 501.2020\n",
      "Epoch 610, Training loss 501.1639\n",
      "Epoch 620, Training loss 501.1244\n",
      "Epoch 630, Training loss 501.0852\n",
      "Epoch 640, Training loss 501.0460\n",
      "Epoch 650, Training loss 501.0063\n",
      "Epoch 660, Training loss 500.9675\n",
      "Epoch 670, Training loss 500.9268\n",
      "Epoch 680, Training loss 500.8858\n",
      "Epoch 690, Training loss 500.8440\n",
      "Epoch 700, Training loss 500.8026\n",
      "Epoch 710, Training loss 500.7608\n",
      "Epoch 720, Training loss 500.7190\n",
      "Epoch 730, Training loss 500.6764\n",
      "Epoch 740, Training loss 500.6330\n",
      "Epoch 750, Training loss 500.5904\n",
      "Epoch 760, Training loss 500.5463\n",
      "Epoch 770, Training loss 500.5021\n",
      "Epoch 780, Training loss 500.4580\n",
      "Epoch 790, Training loss 500.4133\n",
      "Epoch 800, Training loss 500.3683\n",
      "Epoch 810, Training loss 500.3235\n",
      "Epoch 820, Training loss 500.2777\n",
      "Epoch 830, Training loss 500.2320\n",
      "Epoch 840, Training loss 500.1844\n",
      "Epoch 850, Training loss 500.1388\n",
      "Epoch 860, Training loss 500.0917\n",
      "Epoch 870, Training loss 500.0428\n",
      "Epoch 880, Training loss 499.9945\n",
      "Epoch 890, Training loss 499.9451\n",
      "Epoch 900, Training loss 499.8963\n",
      "Epoch 910, Training loss 499.8460\n",
      "Epoch 920, Training loss 499.7979\n",
      "Epoch 930, Training loss 499.7472\n",
      "Epoch 940, Training loss 499.6972\n",
      "Epoch 950, Training loss 499.6475\n",
      "Epoch 960, Training loss 499.5957\n",
      "Epoch 970, Training loss 499.5438\n",
      "Epoch 980, Training loss 499.4920\n",
      "Epoch 990, Training loss 499.4402\n",
      "Epoch 1000, Training loss 499.3875\n",
      "Epoch 1010, Training loss 499.3346\n",
      "Epoch 1020, Training loss 499.2819\n",
      "Epoch 1030, Training loss 499.2285\n",
      "Epoch 1040, Training loss 499.1754\n",
      "Epoch 1050, Training loss 499.1208\n",
      "Epoch 1060, Training loss 499.0665\n",
      "Epoch 1070, Training loss 499.0120\n",
      "Epoch 1080, Training loss 498.9576\n",
      "Epoch 1090, Training loss 498.9023\n",
      "Epoch 1100, Training loss 498.8481\n",
      "Epoch 1110, Training loss 498.7910\n",
      "Epoch 1120, Training loss 498.7349\n",
      "Epoch 1130, Training loss 498.6784\n",
      "Epoch 1140, Training loss 498.6214\n",
      "Epoch 1150, Training loss 498.5654\n",
      "Epoch 1160, Training loss 498.5073\n",
      "Epoch 1170, Training loss 498.4495\n",
      "Epoch 1180, Training loss 498.3919\n",
      "Epoch 1190, Training loss 498.3335\n",
      "Epoch 1200, Training loss 498.2757\n",
      "Epoch 1210, Training loss 498.2166\n",
      "Epoch 1220, Training loss 498.1569\n",
      "Epoch 1230, Training loss 498.0983\n",
      "Epoch 1240, Training loss 498.0376\n",
      "Epoch 1250, Training loss 497.9778\n",
      "Epoch 1260, Training loss 497.9170\n",
      "Epoch 1270, Training loss 497.8574\n",
      "Epoch 1280, Training loss 497.7961\n",
      "Epoch 1290, Training loss 497.7344\n",
      "Epoch 1300, Training loss 497.6726\n",
      "Epoch 1310, Training loss 497.6103\n",
      "Epoch 1320, Training loss 497.5490\n",
      "Epoch 1330, Training loss 497.4869\n",
      "Epoch 1340, Training loss 497.4247\n",
      "Epoch 1350, Training loss 497.3623\n",
      "Epoch 1360, Training loss 497.2973\n",
      "Epoch 1370, Training loss 497.2354\n",
      "Epoch 1380, Training loss 497.1715\n",
      "Epoch 1390, Training loss 497.1050\n",
      "Epoch 1400, Training loss 497.0418\n",
      "Epoch 1410, Training loss 496.9783\n",
      "Epoch 1420, Training loss 496.9140\n",
      "Epoch 1430, Training loss 496.8448\n",
      "Epoch 1440, Training loss 496.7780\n",
      "Epoch 1450, Training loss 496.7129\n",
      "Epoch 1460, Training loss 496.6484\n",
      "Epoch 1470, Training loss 496.5796\n",
      "Epoch 1480, Training loss 496.5130\n",
      "Epoch 1490, Training loss 496.4453\n",
      "Epoch 1500, Training loss 496.3783\n",
      "Epoch 1510, Training loss 496.3102\n",
      "Epoch 1520, Training loss 496.2421\n",
      "Epoch 1530, Training loss 496.1747\n",
      "Epoch 1540, Training loss 496.1038\n",
      "Epoch 1550, Training loss 496.0357\n",
      "Epoch 1560, Training loss 495.9681\n",
      "Epoch 1570, Training loss 495.8966\n",
      "Epoch 1580, Training loss 495.8284\n",
      "Epoch 1590, Training loss 495.7580\n",
      "Epoch 1600, Training loss 495.6855\n",
      "Epoch 1610, Training loss 495.6161\n",
      "Epoch 1620, Training loss 495.5461\n",
      "Epoch 1630, Training loss 495.4728\n",
      "Epoch 1640, Training loss 495.4032\n",
      "Epoch 1650, Training loss 495.3294\n",
      "Epoch 1660, Training loss 495.2573\n",
      "Epoch 1670, Training loss 495.1845\n",
      "Epoch 1680, Training loss 495.1134\n",
      "Epoch 1690, Training loss 495.0394\n",
      "Epoch 1700, Training loss 494.9660\n",
      "Epoch 1710, Training loss 494.8941\n",
      "Epoch 1720, Training loss 494.8187\n",
      "Epoch 1730, Training loss 494.7444\n",
      "Epoch 1740, Training loss 494.6715\n",
      "Epoch 1750, Training loss 494.5958\n",
      "Epoch 1760, Training loss 494.5211\n",
      "Epoch 1770, Training loss 494.4464\n",
      "Epoch 1780, Training loss 494.3699\n",
      "Epoch 1790, Training loss 494.2939\n",
      "Epoch 1800, Training loss 494.2187\n",
      "Epoch 1810, Training loss 494.1451\n",
      "Epoch 1820, Training loss 494.0664\n",
      "Epoch 1830, Training loss 493.9884\n",
      "Epoch 1840, Training loss 493.9114\n",
      "Epoch 1850, Training loss 493.8348\n",
      "Epoch 1860, Training loss 493.7567\n",
      "Epoch 1870, Training loss 493.6794\n",
      "Epoch 1880, Training loss 493.6011\n",
      "Epoch 1890, Training loss 493.5229\n",
      "Epoch 1900, Training loss 493.4446\n",
      "Epoch 1910, Training loss 493.3701\n",
      "Epoch 1920, Training loss 493.2882\n",
      "Epoch 1930, Training loss 493.2064\n",
      "Epoch 1940, Training loss 493.1268\n",
      "Epoch 1950, Training loss 493.0461\n",
      "Epoch 1960, Training loss 492.9660\n",
      "Epoch 1970, Training loss 492.8878\n",
      "Epoch 1980, Training loss 493.3234\n",
      "Epoch 1990, Training loss 614.9834\n",
      "Epoch 2000, Training loss 493.8276\n",
      "Epoch 2010, Training loss 496.8264\n",
      "Epoch 2020, Training loss 497.2459\n",
      "Epoch 2030, Training loss 494.5077\n",
      "Epoch 2040, Training loss 492.9307\n",
      "Epoch 2050, Training loss 492.4402\n",
      "Epoch 2060, Training loss 492.2782\n",
      "Epoch 2070, Training loss 492.1849\n",
      "Epoch 2080, Training loss 492.1050\n",
      "Epoch 2090, Training loss 492.0267\n",
      "Epoch 2100, Training loss 491.9508\n",
      "Epoch 2110, Training loss 491.8758\n",
      "Epoch 2120, Training loss 491.8019\n",
      "Epoch 2130, Training loss 491.7276\n",
      "Epoch 2140, Training loss 491.6530\n",
      "Epoch 2150, Training loss 491.5783\n",
      "Epoch 2160, Training loss 491.5045\n",
      "Epoch 2170, Training loss 491.4284\n",
      "Epoch 2180, Training loss 491.3528\n",
      "Epoch 2190, Training loss 491.2771\n",
      "Epoch 2200, Training loss 491.2014\n",
      "Epoch 2210, Training loss 491.1258\n",
      "Epoch 2220, Training loss 491.0494\n",
      "Epoch 2230, Training loss 490.9722\n",
      "Epoch 2240, Training loss 490.8957\n",
      "Epoch 2250, Training loss 490.8284\n",
      "Epoch 2260, Training loss 491.1103\n",
      "Epoch 2270, Training loss 516.4820\n",
      "Epoch 2280, Training loss 503.3424\n",
      "Epoch 2290, Training loss 497.2650\n",
      "Epoch 2300, Training loss 494.0074\n",
      "Epoch 2310, Training loss 490.4596\n",
      "Epoch 2320, Training loss 490.8973\n",
      "Epoch 2330, Training loss 490.2545\n",
      "Epoch 2340, Training loss 490.2187\n",
      "Epoch 2350, Training loss 490.1199\n",
      "Epoch 2360, Training loss 490.0238\n",
      "Epoch 2370, Training loss 489.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2380, Training loss 489.8721\n",
      "Epoch 2390, Training loss 489.7975\n",
      "Epoch 2400, Training loss 489.7227\n",
      "Epoch 2410, Training loss 489.6480\n",
      "Epoch 2420, Training loss 489.5737\n",
      "Epoch 2430, Training loss 489.5012\n",
      "Epoch 2440, Training loss 489.4792\n",
      "Epoch 2450, Training loss 491.8908\n",
      "Epoch 2460, Training loss 558.4195\n",
      "Epoch 2470, Training loss 496.1207\n",
      "Epoch 2480, Training loss 495.0084\n",
      "Epoch 2490, Training loss 489.4211\n",
      "Epoch 2500, Training loss 489.8700\n",
      "Epoch 2510, Training loss 489.0400\n",
      "Epoch 2520, Training loss 488.8915\n",
      "Epoch 2530, Training loss 488.8337\n",
      "Epoch 2540, Training loss 488.7287\n",
      "Epoch 2550, Training loss 488.6444\n",
      "Epoch 2560, Training loss 488.5691\n",
      "Epoch 2570, Training loss 488.4951\n",
      "Epoch 2580, Training loss 488.4208\n",
      "Epoch 2590, Training loss 488.3476\n",
      "Epoch 2600, Training loss 488.2737\n",
      "Epoch 2610, Training loss 488.1999\n",
      "Epoch 2620, Training loss 488.1260\n",
      "Epoch 2630, Training loss 488.0578\n",
      "Epoch 2640, Training loss 488.7390\n",
      "Epoch 2650, Training loss 588.2588\n",
      "Epoch 2660, Training loss 495.3973\n",
      "Epoch 2670, Training loss 494.3686\n",
      "Epoch 2680, Training loss 490.6923\n",
      "Epoch 2690, Training loss 489.0206\n",
      "Epoch 2700, Training loss 488.2037\n",
      "Epoch 2710, Training loss 487.7258\n",
      "Epoch 2720, Training loss 487.4727\n",
      "Epoch 2730, Training loss 487.3790\n",
      "Epoch 2740, Training loss 487.3186\n",
      "Epoch 2750, Training loss 487.2406\n",
      "Epoch 2760, Training loss 487.1715\n",
      "Epoch 2770, Training loss 487.1006\n",
      "Epoch 2780, Training loss 487.0317\n",
      "Epoch 2790, Training loss 486.9610\n",
      "Epoch 2800, Training loss 486.8912\n",
      "Epoch 2810, Training loss 486.8217\n",
      "Epoch 2820, Training loss 486.7506\n",
      "Epoch 2830, Training loss 486.6801\n",
      "Epoch 2840, Training loss 486.6090\n",
      "Epoch 2850, Training loss 486.5373\n",
      "Epoch 2860, Training loss 486.4658\n",
      "Epoch 2870, Training loss 486.3955\n",
      "Epoch 2880, Training loss 486.3306\n",
      "Epoch 2890, Training loss 486.5081\n",
      "Epoch 2900, Training loss 506.7024\n",
      "Epoch 2910, Training loss 491.2699\n",
      "Epoch 2920, Training loss 496.9880\n",
      "Epoch 2930, Training loss 490.1842\n",
      "Epoch 2940, Training loss 485.9256\n",
      "Epoch 2950, Training loss 486.5104\n",
      "Epoch 2960, Training loss 485.8257\n",
      "Epoch 2970, Training loss 485.8116\n",
      "Epoch 2980, Training loss 485.6545\n",
      "Epoch 2990, Training loss 485.5918\n",
      "Epoch 3000, Training loss 485.5211\n",
      "Epoch 3010, Training loss 485.4501\n",
      "Epoch 3020, Training loss 485.3796\n",
      "Epoch 3030, Training loss 485.3109\n",
      "Epoch 3040, Training loss 485.2412\n",
      "Epoch 3050, Training loss 485.1723\n",
      "Epoch 3060, Training loss 485.1026\n",
      "Epoch 3070, Training loss 485.0331\n",
      "Epoch 3080, Training loss 484.9639\n",
      "Epoch 3090, Training loss 484.9496\n",
      "Epoch 3100, Training loss 488.5383\n",
      "Epoch 3110, Training loss 554.7494\n",
      "Epoch 3120, Training loss 485.0889\n",
      "Epoch 3130, Training loss 490.5517\n",
      "Epoch 3140, Training loss 486.6493\n",
      "Epoch 3150, Training loss 484.5624\n",
      "Epoch 3160, Training loss 484.8247\n",
      "Epoch 3170, Training loss 484.3771\n",
      "Epoch 3180, Training loss 484.3499\n",
      "Epoch 3190, Training loss 484.2540\n",
      "Epoch 3200, Training loss 484.1755\n",
      "Epoch 3210, Training loss 484.1092\n",
      "Epoch 3220, Training loss 484.0415\n",
      "Epoch 3230, Training loss 483.9738\n",
      "Epoch 3240, Training loss 483.9063\n",
      "Epoch 3250, Training loss 483.8388\n",
      "Epoch 3260, Training loss 483.7706\n",
      "Epoch 3270, Training loss 483.7026\n",
      "Epoch 3280, Training loss 483.6354\n",
      "Epoch 3290, Training loss 483.5740\n",
      "Epoch 3300, Training loss 483.8868\n",
      "Epoch 3310, Training loss 518.1744\n",
      "Epoch 3320, Training loss 510.7334\n",
      "Epoch 3330, Training loss 484.2204\n",
      "Epoch 3340, Training loss 487.7489\n",
      "Epoch 3350, Training loss 484.3895\n",
      "Epoch 3360, Training loss 483.1586\n",
      "Epoch 3370, Training loss 483.3080\n",
      "Epoch 3380, Training loss 482.9980\n",
      "Epoch 3390, Training loss 482.9659\n",
      "Epoch 3400, Training loss 482.8751\n",
      "Epoch 3410, Training loss 482.8040\n",
      "Epoch 3420, Training loss 482.7384\n",
      "Epoch 3430, Training loss 482.6734\n",
      "Epoch 3440, Training loss 482.6097\n",
      "Epoch 3450, Training loss 482.5406\n",
      "Epoch 3460, Training loss 482.4759\n",
      "Epoch 3470, Training loss 482.4120\n",
      "Epoch 3480, Training loss 482.3429\n",
      "Epoch 3490, Training loss 482.2763\n",
      "Epoch 3500, Training loss 482.2121\n",
      "Epoch 3510, Training loss 482.1540\n",
      "Epoch 3520, Training loss 483.1646\n",
      "Epoch 3530, Training loss 573.6573\n",
      "Epoch 3540, Training loss 497.5203\n",
      "Epoch 3550, Training loss 494.2886\n",
      "Epoch 3560, Training loss 485.0632\n",
      "Epoch 3570, Training loss 481.9307\n",
      "Epoch 3580, Training loss 481.8706\n",
      "Epoch 3590, Training loss 481.8703\n",
      "Epoch 3600, Training loss 481.6035\n",
      "Epoch 3610, Training loss 481.5532\n",
      "Epoch 3620, Training loss 481.4685\n",
      "Epoch 3630, Training loss 481.4093\n",
      "Epoch 3640, Training loss 481.3447\n",
      "Epoch 3650, Training loss 481.2792\n",
      "Epoch 3660, Training loss 481.2156\n",
      "Epoch 3670, Training loss 481.1537\n",
      "Epoch 3680, Training loss 481.0890\n",
      "Epoch 3690, Training loss 481.0252\n",
      "Epoch 3700, Training loss 480.9617\n",
      "Epoch 3710, Training loss 480.8968\n",
      "Epoch 3720, Training loss 480.8320\n",
      "Epoch 3730, Training loss 480.7675\n",
      "Epoch 3740, Training loss 480.7030\n",
      "Epoch 3750, Training loss 480.6981\n",
      "Epoch 3760, Training loss 487.4109\n",
      "Epoch 3770, Training loss 509.2635\n",
      "Epoch 3780, Training loss 503.7567\n",
      "Epoch 3790, Training loss 483.8241\n",
      "Epoch 3800, Training loss 480.3580\n",
      "Epoch 3810, Training loss 480.7064\n",
      "Epoch 3820, Training loss 480.6228\n",
      "Epoch 3830, Training loss 480.2002\n",
      "Epoch 3840, Training loss 480.1307\n",
      "Epoch 3850, Training loss 480.0561\n",
      "Epoch 3860, Training loss 479.9872\n",
      "Epoch 3870, Training loss 479.9226\n",
      "Epoch 3880, Training loss 479.8619\n",
      "Epoch 3890, Training loss 479.7996\n",
      "Epoch 3900, Training loss 479.7373\n",
      "Epoch 3910, Training loss 479.6751\n",
      "Epoch 3920, Training loss 479.6128\n",
      "Epoch 3930, Training loss 479.5505\n",
      "Epoch 3940, Training loss 479.4876\n",
      "Epoch 3950, Training loss 479.4246\n",
      "Epoch 3960, Training loss 479.3614\n",
      "Epoch 3970, Training loss 479.2980\n",
      "Epoch 3980, Training loss 479.2373\n",
      "Epoch 3990, Training loss 479.3342\n",
      "Epoch 4000, Training loss 497.4392\n",
      "Epoch 4010, Training loss 483.5633\n",
      "Epoch 4020, Training loss 488.0355\n",
      "Epoch 4030, Training loss 485.6882\n",
      "Epoch 4040, Training loss 480.7179\n",
      "Epoch 4050, Training loss 478.8764\n",
      "Epoch 4060, Training loss 478.9245\n",
      "Epoch 4070, Training loss 478.8182\n",
      "Epoch 4080, Training loss 478.6561\n",
      "Epoch 4090, Training loss 478.6096\n",
      "Epoch 4100, Training loss 478.5367\n",
      "Epoch 4110, Training loss 478.4766\n",
      "Epoch 4120, Training loss 478.4167\n",
      "Epoch 4130, Training loss 478.3560\n",
      "Epoch 4140, Training loss 478.2957\n",
      "Epoch 4150, Training loss 478.2349\n",
      "Epoch 4160, Training loss 478.1741\n",
      "Epoch 4170, Training loss 478.1132\n",
      "Epoch 4180, Training loss 478.0521\n",
      "Epoch 4190, Training loss 477.9906\n",
      "Epoch 4200, Training loss 477.9308\n",
      "Epoch 4210, Training loss 477.8705\n",
      "Epoch 4220, Training loss 477.8584\n",
      "Epoch 4230, Training loss 481.7764\n",
      "Epoch 4240, Training loss 551.2206\n",
      "Epoch 4250, Training loss 481.1713\n",
      "Epoch 4260, Training loss 480.3488\n",
      "Epoch 4270, Training loss 480.7077\n",
      "Epoch 4280, Training loss 477.8288\n",
      "Epoch 4290, Training loss 477.5551\n",
      "Epoch 4300, Training loss 477.4812\n",
      "Epoch 4310, Training loss 477.3151\n",
      "Epoch 4320, Training loss 477.2515\n",
      "Epoch 4330, Training loss 477.1915\n",
      "Epoch 4340, Training loss 477.1261\n",
      "Epoch 4350, Training loss 477.0671\n",
      "Epoch 4360, Training loss 477.0081\n",
      "Epoch 4370, Training loss 476.9491\n",
      "Epoch 4380, Training loss 476.8906\n",
      "Epoch 4390, Training loss 476.8310\n",
      "Epoch 4400, Training loss 476.7721\n",
      "Epoch 4410, Training loss 476.7120\n",
      "Epoch 4420, Training loss 476.6520\n",
      "Epoch 4430, Training loss 476.5928\n",
      "Epoch 4440, Training loss 476.5523\n",
      "Epoch 4450, Training loss 478.2006\n",
      "Epoch 4460, Training loss 579.7302\n",
      "Epoch 4470, Training loss 479.5157\n",
      "Epoch 4480, Training loss 485.6810\n",
      "Epoch 4490, Training loss 480.0894\n",
      "Epoch 4500, Training loss 476.5672\n",
      "Epoch 4510, Training loss 476.2510\n",
      "Epoch 4520, Training loss 476.2955\n",
      "Epoch 4530, Training loss 476.0487\n",
      "Epoch 4540, Training loss 476.0141\n",
      "Epoch 4550, Training loss 475.9320\n",
      "Epoch 4560, Training loss 475.8764\n",
      "Epoch 4570, Training loss 475.8198\n",
      "Epoch 4580, Training loss 475.7611\n",
      "Epoch 4590, Training loss 475.7057\n",
      "Epoch 4600, Training loss 475.6487\n",
      "Epoch 4610, Training loss 475.5910\n",
      "Epoch 4620, Training loss 475.5348\n",
      "Epoch 4630, Training loss 475.4782\n",
      "Epoch 4640, Training loss 475.4197\n",
      "Epoch 4650, Training loss 475.3591\n",
      "Epoch 4660, Training loss 475.3011\n",
      "Epoch 4670, Training loss 475.2492\n",
      "Epoch 4680, Training loss 475.5283\n",
      "Epoch 4690, Training loss 510.9644\n",
      "Epoch 4700, Training loss 508.0084\n",
      "Epoch 4710, Training loss 475.0941\n",
      "Epoch 4720, Training loss 478.1403\n",
      "Epoch 4730, Training loss 477.0591\n",
      "Epoch 4740, Training loss 475.1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4750, Training loss 474.8914\n",
      "Epoch 4760, Training loss 474.8710\n",
      "Epoch 4770, Training loss 474.7153\n",
      "Epoch 4780, Training loss 474.6739\n",
      "Epoch 4790, Training loss 474.6074\n",
      "Epoch 4800, Training loss 474.5499\n",
      "Epoch 4810, Training loss 474.4950\n",
      "Epoch 4820, Training loss 474.4402\n",
      "Epoch 4830, Training loss 474.3837\n",
      "Epoch 4840, Training loss 474.3288\n",
      "Epoch 4850, Training loss 474.2769\n",
      "Epoch 4860, Training loss 474.2232\n",
      "Epoch 4870, Training loss 474.1610\n",
      "Epoch 4880, Training loss 474.1035\n",
      "Epoch 4890, Training loss 474.0472\n",
      "Epoch 4900, Training loss 473.9988\n",
      "Epoch 4910, Training loss 474.2832\n",
      "Epoch 4920, Training loss 507.2540\n",
      "Epoch 4930, Training loss 499.9514\n",
      "Epoch 4940, Training loss 475.3518\n",
      "Epoch 4950, Training loss 478.7574\n",
      "Epoch 4960, Training loss 475.0955\n",
      "Epoch 4970, Training loss 473.6420\n",
      "Epoch 4980, Training loss 473.8371\n",
      "Epoch 4990, Training loss 473.5355\n",
      "Epoch 5000, Training loss 473.5043\n",
      "Epoch 5010, Training loss 473.4205\n",
      "Epoch 5020, Training loss 473.3692\n",
      "Epoch 5030, Training loss 473.3142\n",
      "Epoch 5040, Training loss 473.2610\n",
      "Epoch 5050, Training loss 473.2061\n",
      "Epoch 5060, Training loss 473.1531\n",
      "Epoch 5070, Training loss 473.0983\n",
      "Epoch 5080, Training loss 473.0446\n",
      "Epoch 5090, Training loss 472.9887\n",
      "Epoch 5100, Training loss 472.9354\n",
      "Epoch 5110, Training loss 472.8859\n",
      "Epoch 5120, Training loss 472.9208\n",
      "Epoch 5130, Training loss 476.8120\n",
      "Epoch 5140, Training loss 537.6214\n",
      "Epoch 5150, Training loss 476.7221\n",
      "Epoch 5160, Training loss 477.9578\n",
      "Epoch 5170, Training loss 473.4888\n",
      "Epoch 5180, Training loss 472.9379\n",
      "Epoch 5190, Training loss 472.8122\n",
      "Epoch 5200, Training loss 472.4476\n",
      "Epoch 5210, Training loss 472.3674\n",
      "Epoch 5220, Training loss 472.3186\n",
      "Epoch 5230, Training loss 472.2679\n",
      "Epoch 5240, Training loss 472.2092\n",
      "Epoch 5250, Training loss 472.1541\n",
      "Epoch 5260, Training loss 472.1060\n",
      "Epoch 5270, Training loss 472.0471\n",
      "Epoch 5280, Training loss 472.0042\n",
      "Epoch 5290, Training loss 471.9729\n",
      "Epoch 5300, Training loss 472.9880\n",
      "Epoch 5310, Training loss 510.0914\n",
      "Epoch 5320, Training loss 485.3470\n",
      "Epoch 5330, Training loss 480.5937\n",
      "Epoch 5340, Training loss 471.8829\n",
      "Epoch 5350, Training loss 472.1722\n",
      "Epoch 5360, Training loss 472.0262\n",
      "Epoch 5370, Training loss 471.6802\n",
      "Epoch 5380, Training loss 471.5264\n",
      "Epoch 5390, Training loss 471.4519\n",
      "Epoch 5400, Training loss 471.3925\n",
      "Epoch 5410, Training loss 471.3315\n",
      "Epoch 5420, Training loss 471.2792\n",
      "Epoch 5430, Training loss 471.2258\n",
      "Epoch 5440, Training loss 471.1720\n",
      "Epoch 5450, Training loss 471.1241\n",
      "Epoch 5460, Training loss 471.2445\n",
      "Epoch 5470, Training loss 484.7467\n",
      "Epoch 5480, Training loss 472.7446\n",
      "Epoch 5490, Training loss 489.4008\n",
      "Epoch 5500, Training loss 471.5363\n",
      "Epoch 5510, Training loss 472.2022\n",
      "Epoch 5520, Training loss 471.3553\n",
      "Epoch 5530, Training loss 470.8331\n",
      "Epoch 5540, Training loss 470.7459\n",
      "Epoch 5550, Training loss 470.6655\n",
      "Epoch 5560, Training loss 470.5830\n",
      "Epoch 5570, Training loss 470.5365\n",
      "Epoch 5580, Training loss 470.4848\n",
      "Epoch 5590, Training loss 470.4320\n",
      "Epoch 5600, Training loss 470.3816\n",
      "Epoch 5610, Training loss 470.3324\n",
      "Epoch 5620, Training loss 470.2801\n",
      "Epoch 5630, Training loss 470.2310\n",
      "Epoch 5640, Training loss 470.1867\n",
      "Epoch 5650, Training loss 470.2098\n",
      "Epoch 5660, Training loss 471.6079\n",
      "Epoch 5670, Training loss 509.2478\n",
      "Epoch 5680, Training loss 481.6969\n",
      "Epoch 5690, Training loss 477.7387\n",
      "Epoch 5700, Training loss 471.5559\n",
      "Epoch 5710, Training loss 469.9233\n",
      "Epoch 5720, Training loss 469.7890\n",
      "Epoch 5730, Training loss 469.7390\n",
      "Epoch 5740, Training loss 469.6972\n",
      "Epoch 5750, Training loss 469.6594\n",
      "Epoch 5760, Training loss 469.5975\n",
      "Epoch 5770, Training loss 469.5441\n",
      "Epoch 5780, Training loss 469.4892\n",
      "Epoch 5790, Training loss 469.4545\n",
      "Epoch 5800, Training loss 469.7624\n",
      "Epoch 5810, Training loss 486.1165\n",
      "Epoch 5820, Training loss 470.3445\n",
      "Epoch 5830, Training loss 478.5510\n",
      "Epoch 5840, Training loss 471.9486\n",
      "Epoch 5850, Training loss 469.5016\n",
      "Epoch 5860, Training loss 469.7897\n",
      "Epoch 5870, Training loss 469.1729\n",
      "Epoch 5880, Training loss 469.0197\n",
      "Epoch 5890, Training loss 468.9743\n",
      "Epoch 5900, Training loss 468.9225\n",
      "Epoch 5910, Training loss 468.8754\n",
      "Epoch 5920, Training loss 468.8299\n",
      "Epoch 5930, Training loss 468.7770\n",
      "Epoch 5940, Training loss 468.7278\n",
      "Epoch 5950, Training loss 468.6801\n",
      "Epoch 5960, Training loss 468.6511\n",
      "Epoch 5970, Training loss 469.0921\n",
      "Epoch 5980, Training loss 493.2550\n",
      "Epoch 5990, Training loss 471.3883\n",
      "Epoch 6000, Training loss 481.2724\n",
      "Epoch 6010, Training loss 468.8153\n",
      "Epoch 6020, Training loss 469.5815\n",
      "Epoch 6030, Training loss 468.7624\n",
      "Epoch 6040, Training loss 468.2732\n",
      "Epoch 6050, Training loss 468.2425\n",
      "Epoch 6060, Training loss 468.1936\n",
      "Epoch 6070, Training loss 468.1346\n",
      "Epoch 6080, Training loss 468.0818\n",
      "Epoch 6090, Training loss 468.0345\n",
      "Epoch 6100, Training loss 467.9893\n",
      "Epoch 6110, Training loss 467.9411\n",
      "Epoch 6120, Training loss 467.8969\n",
      "Epoch 6130, Training loss 467.8983\n",
      "Epoch 6140, Training loss 469.0758\n",
      "Epoch 6150, Training loss 510.6300\n",
      "Epoch 6160, Training loss 485.1669\n",
      "Epoch 6170, Training loss 475.6347\n",
      "Epoch 6180, Training loss 467.6645\n",
      "Epoch 6190, Training loss 468.2540\n",
      "Epoch 6200, Training loss 467.9917\n",
      "Epoch 6210, Training loss 467.6281\n",
      "Epoch 6220, Training loss 467.4867\n",
      "Epoch 6230, Training loss 467.4153\n",
      "Epoch 6240, Training loss 467.3578\n",
      "Epoch 6250, Training loss 467.3066\n",
      "Epoch 6260, Training loss 467.2621\n",
      "Epoch 6270, Training loss 467.2122\n",
      "Epoch 6280, Training loss 467.1763\n",
      "Epoch 6290, Training loss 467.2234\n",
      "Epoch 6300, Training loss 470.8495\n",
      "Epoch 6310, Training loss 532.2453\n",
      "Epoch 6320, Training loss 476.4830\n",
      "Epoch 6330, Training loss 469.3263\n",
      "Epoch 6340, Training loss 469.3556\n",
      "Epoch 6350, Training loss 466.8666\n",
      "Epoch 6360, Training loss 467.0640\n",
      "Epoch 6370, Training loss 466.9187\n",
      "Epoch 6380, Training loss 466.7722\n",
      "Epoch 6390, Training loss 466.7043\n",
      "Epoch 6400, Training loss 466.6478\n",
      "Epoch 6410, Training loss 466.5997\n",
      "Epoch 6420, Training loss 466.5518\n",
      "Epoch 6430, Training loss 466.5068\n",
      "Epoch 6440, Training loss 466.4613\n",
      "Epoch 6450, Training loss 466.4155\n",
      "Epoch 6460, Training loss 466.3703\n",
      "Epoch 6470, Training loss 466.3463\n",
      "Epoch 6480, Training loss 468.9852\n",
      "Epoch 6490, Training loss 569.4157\n",
      "Epoch 6500, Training loss 467.1872\n",
      "Epoch 6510, Training loss 469.9946\n",
      "Epoch 6520, Training loss 469.8753\n",
      "Epoch 6530, Training loss 467.0627\n",
      "Epoch 6540, Training loss 466.0475\n",
      "Epoch 6550, Training loss 466.1548\n",
      "Epoch 6560, Training loss 465.9887\n",
      "Epoch 6570, Training loss 465.9350\n",
      "Epoch 6580, Training loss 465.8822\n",
      "Epoch 6590, Training loss 465.8400\n",
      "Epoch 6600, Training loss 465.7952\n",
      "Epoch 6610, Training loss 465.7524\n",
      "Epoch 6620, Training loss 465.7099\n",
      "Epoch 6630, Training loss 465.6668\n",
      "Epoch 6640, Training loss 465.6239\n",
      "Epoch 6650, Training loss 465.5827\n",
      "Epoch 6660, Training loss 465.5377\n",
      "Epoch 6670, Training loss 465.4946\n",
      "Epoch 6680, Training loss 465.4513\n",
      "Epoch 6690, Training loss 465.4090\n",
      "Epoch 6700, Training loss 465.3688\n",
      "Epoch 6710, Training loss 465.4569\n",
      "Epoch 6720, Training loss 483.2262\n",
      "Epoch 6730, Training loss 469.2444\n",
      "Epoch 6740, Training loss 476.1036\n",
      "Epoch 6750, Training loss 472.6411\n",
      "Epoch 6760, Training loss 467.1801\n",
      "Epoch 6770, Training loss 465.1444\n",
      "Epoch 6780, Training loss 465.2291\n",
      "Epoch 6790, Training loss 465.1426\n",
      "Epoch 6800, Training loss 464.9797\n",
      "Epoch 6810, Training loss 464.9625\n",
      "Epoch 6820, Training loss 464.8988\n",
      "Epoch 6830, Training loss 464.8585\n",
      "Epoch 6840, Training loss 464.8198\n",
      "Epoch 6850, Training loss 464.7761\n",
      "Epoch 6860, Training loss 464.7375\n",
      "Epoch 6870, Training loss 464.6943\n",
      "Epoch 6880, Training loss 464.6548\n",
      "Epoch 6890, Training loss 464.6172\n",
      "Epoch 6900, Training loss 464.5743\n",
      "Epoch 6910, Training loss 464.5308\n",
      "Epoch 6920, Training loss 464.4945\n",
      "Epoch 6930, Training loss 464.4505\n",
      "Epoch 6940, Training loss 464.4193\n",
      "Epoch 6950, Training loss 465.3752\n",
      "Epoch 6960, Training loss 543.6967\n",
      "Epoch 6970, Training loss 496.1929\n",
      "Epoch 6980, Training loss 470.4909\n",
      "Epoch 6990, Training loss 464.5580\n",
      "Epoch 7000, Training loss 465.8068\n",
      "Epoch 7010, Training loss 464.2469\n",
      "Epoch 7020, Training loss 464.2886\n",
      "Epoch 7030, Training loss 464.0817\n",
      "Epoch 7040, Training loss 464.0622\n",
      "Epoch 7050, Training loss 463.9959\n",
      "Epoch 7060, Training loss 463.9533\n",
      "Epoch 7070, Training loss 463.9135\n",
      "Epoch 7080, Training loss 463.8740\n",
      "Epoch 7090, Training loss 463.8346\n",
      "Epoch 7100, Training loss 463.7948\n",
      "Epoch 7110, Training loss 463.7551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7120, Training loss 463.7152\n",
      "Epoch 7130, Training loss 463.6752\n",
      "Epoch 7140, Training loss 463.6351\n",
      "Epoch 7150, Training loss 463.5951\n",
      "Epoch 7160, Training loss 463.5622\n",
      "Epoch 7170, Training loss 464.2395\n",
      "Epoch 7180, Training loss 551.6522\n",
      "Epoch 7190, Training loss 495.3797\n",
      "Epoch 7200, Training loss 478.0332\n",
      "Epoch 7210, Training loss 466.0843\n",
      "Epoch 7220, Training loss 463.3998\n",
      "Epoch 7230, Training loss 463.5686\n",
      "Epoch 7240, Training loss 463.5250\n",
      "Epoch 7250, Training loss 463.2513\n",
      "Epoch 7260, Training loss 463.2336\n",
      "Epoch 7270, Training loss 463.1667\n",
      "Epoch 7280, Training loss 463.1299\n",
      "Epoch 7290, Training loss 463.0902\n",
      "Epoch 7300, Training loss 463.0507\n",
      "Epoch 7310, Training loss 463.0128\n",
      "Epoch 7320, Training loss 462.9748\n",
      "Epoch 7330, Training loss 462.9369\n",
      "Epoch 7340, Training loss 462.8989\n",
      "Epoch 7350, Training loss 462.8608\n",
      "Epoch 7360, Training loss 462.8226\n",
      "Epoch 7370, Training loss 462.7843\n",
      "Epoch 7380, Training loss 462.7459\n",
      "Epoch 7390, Training loss 462.7074\n",
      "Epoch 7400, Training loss 462.6689\n",
      "Epoch 7410, Training loss 462.6388\n",
      "Epoch 7420, Training loss 463.7008\n",
      "Epoch 7430, Training loss 588.1729\n",
      "Epoch 7440, Training loss 464.5136\n",
      "Epoch 7450, Training loss 468.2072\n",
      "Epoch 7460, Training loss 465.9514\n",
      "Epoch 7470, Training loss 464.2099\n",
      "Epoch 7480, Training loss 463.0875\n",
      "Epoch 7490, Training loss 462.4677\n",
      "Epoch 7500, Training loss 462.3299\n",
      "Epoch 7510, Training loss 462.3174\n",
      "Epoch 7520, Training loss 462.2631\n",
      "Epoch 7530, Training loss 462.2240\n",
      "Epoch 7540, Training loss 462.1834\n",
      "Epoch 7550, Training loss 462.1492\n",
      "Epoch 7560, Training loss 462.1131\n",
      "Epoch 7570, Training loss 462.0748\n",
      "Epoch 7580, Training loss 462.0399\n",
      "Epoch 7590, Training loss 462.0037\n",
      "Epoch 7600, Training loss 461.9664\n",
      "Epoch 7610, Training loss 461.9307\n",
      "Epoch 7620, Training loss 461.8952\n",
      "Epoch 7630, Training loss 461.8573\n",
      "Epoch 7640, Training loss 461.8185\n",
      "Epoch 7650, Training loss 461.7829\n",
      "Epoch 7660, Training loss 461.7783\n",
      "Epoch 7670, Training loss 462.6682\n",
      "Epoch 7680, Training loss 501.2156\n",
      "Epoch 7690, Training loss 478.4864\n",
      "Epoch 7700, Training loss 471.1509\n",
      "Epoch 7710, Training loss 461.7304\n",
      "Epoch 7720, Training loss 463.0440\n",
      "Epoch 7730, Training loss 461.7033\n",
      "Epoch 7740, Training loss 461.4788\n",
      "Epoch 7750, Training loss 461.4741\n",
      "Epoch 7760, Training loss 461.4213\n",
      "Epoch 7770, Training loss 461.3713\n",
      "Epoch 7780, Training loss 461.3279\n",
      "Epoch 7790, Training loss 461.2900\n",
      "Epoch 7800, Training loss 461.2544\n",
      "Epoch 7810, Training loss 461.2171\n",
      "Epoch 7820, Training loss 461.1813\n",
      "Epoch 7830, Training loss 461.1469\n",
      "Epoch 7840, Training loss 461.1879\n",
      "Epoch 7850, Training loss 465.8829\n",
      "Epoch 7860, Training loss 528.7729\n",
      "Epoch 7870, Training loss 461.1543\n",
      "Epoch 7880, Training loss 468.7485\n",
      "Epoch 7890, Training loss 461.7851\n",
      "Epoch 7900, Training loss 461.5603\n",
      "Epoch 7910, Training loss 461.0291\n",
      "Epoch 7920, Training loss 460.9698\n",
      "Epoch 7930, Training loss 460.8104\n",
      "Epoch 7940, Training loss 460.7845\n",
      "Epoch 7950, Training loss 460.7474\n",
      "Epoch 7960, Training loss 460.7082\n",
      "Epoch 7970, Training loss 460.6709\n",
      "Epoch 7980, Training loss 460.6346\n",
      "Epoch 7990, Training loss 460.5990\n",
      "Epoch 8000, Training loss 460.5634\n",
      "Epoch 8010, Training loss 460.5278\n",
      "Epoch 8020, Training loss 460.4922\n",
      "Epoch 8030, Training loss 460.4567\n",
      "Epoch 8040, Training loss 460.4287\n",
      "Epoch 8050, Training loss 460.9052\n",
      "Epoch 8060, Training loss 512.6213\n",
      "Epoch 8070, Training loss 505.8349\n",
      "Epoch 8080, Training loss 461.5577\n",
      "Epoch 8090, Training loss 461.7143\n",
      "Epoch 8100, Training loss 462.2083\n",
      "Epoch 8110, Training loss 460.5601\n",
      "Epoch 8120, Training loss 460.2091\n",
      "Epoch 8130, Training loss 460.2293\n",
      "Epoch 8140, Training loss 460.1030\n",
      "Epoch 8150, Training loss 460.0774\n",
      "Epoch 8160, Training loss 460.0364\n",
      "Epoch 8170, Training loss 460.0002\n",
      "Epoch 8180, Training loss 459.9663\n",
      "Epoch 8190, Training loss 459.9323\n",
      "Epoch 8200, Training loss 459.8983\n",
      "Epoch 8210, Training loss 459.8642\n",
      "Epoch 8220, Training loss 459.8301\n",
      "Epoch 8230, Training loss 459.7959\n",
      "Epoch 8240, Training loss 459.7616\n",
      "Epoch 8250, Training loss 459.7270\n",
      "Epoch 8260, Training loss 459.6937\n",
      "Epoch 8270, Training loss 459.6946\n",
      "Epoch 8280, Training loss 461.6835\n",
      "Epoch 8290, Training loss 550.0700\n",
      "Epoch 8300, Training loss 474.9741\n",
      "Epoch 8310, Training loss 468.0240\n",
      "Epoch 8320, Training loss 459.5413\n",
      "Epoch 8330, Training loss 460.8178\n",
      "Epoch 8340, Training loss 459.4848\n",
      "Epoch 8350, Training loss 459.5891\n",
      "Epoch 8360, Training loss 459.3779\n",
      "Epoch 8370, Training loss 459.3598\n",
      "Epoch 8380, Training loss 459.3206\n",
      "Epoch 8390, Training loss 459.2808\n",
      "Epoch 8400, Training loss 459.2461\n",
      "Epoch 8410, Training loss 459.2126\n",
      "Epoch 8420, Training loss 459.1793\n",
      "Epoch 8430, Training loss 459.1459\n",
      "Epoch 8440, Training loss 459.1124\n",
      "Epoch 8450, Training loss 459.0790\n",
      "Epoch 8460, Training loss 459.0454\n",
      "Epoch 8470, Training loss 459.0127\n",
      "Epoch 8480, Training loss 459.0120\n",
      "Epoch 8490, Training loss 461.5390\n",
      "Epoch 8500, Training loss 561.8709\n",
      "Epoch 8510, Training loss 460.1899\n",
      "Epoch 8520, Training loss 468.4839\n",
      "Epoch 8530, Training loss 461.7313\n",
      "Epoch 8540, Training loss 458.8041\n",
      "Epoch 8550, Training loss 459.1905\n",
      "Epoch 8560, Training loss 458.8070\n",
      "Epoch 8570, Training loss 458.7551\n",
      "Epoch 8580, Training loss 458.6830\n",
      "Epoch 8590, Training loss 458.6564\n",
      "Epoch 8600, Training loss 458.6176\n",
      "Epoch 8610, Training loss 458.5839\n",
      "Epoch 8620, Training loss 458.5520\n",
      "Epoch 8630, Training loss 458.5204\n",
      "Epoch 8640, Training loss 458.4883\n",
      "Epoch 8650, Training loss 458.4562\n",
      "Epoch 8660, Training loss 458.4242\n",
      "Epoch 8670, Training loss 458.3917\n",
      "Epoch 8680, Training loss 458.3602\n",
      "Epoch 8690, Training loss 458.3289\n",
      "Epoch 8700, Training loss 458.3300\n",
      "Epoch 8710, Training loss 460.0588\n",
      "Epoch 8720, Training loss 543.8675\n",
      "Epoch 8730, Training loss 480.8072\n",
      "Epoch 8740, Training loss 464.1037\n",
      "Epoch 8750, Training loss 458.8117\n",
      "Epoch 8760, Training loss 459.5117\n",
      "Epoch 8770, Training loss 458.0991\n",
      "Epoch 8780, Training loss 458.2435\n",
      "Epoch 8790, Training loss 458.0548\n",
      "Epoch 8800, Training loss 458.0048\n",
      "Epoch 8810, Training loss 457.9787\n",
      "Epoch 8820, Training loss 457.9446\n",
      "Epoch 8830, Training loss 457.9110\n",
      "Epoch 8840, Training loss 457.8787\n",
      "Epoch 8850, Training loss 457.8472\n",
      "Epoch 8860, Training loss 457.8158\n",
      "Epoch 8870, Training loss 457.7844\n",
      "Epoch 8880, Training loss 457.7530\n",
      "Epoch 8890, Training loss 457.7228\n",
      "Epoch 8900, Training loss 457.7044\n",
      "Epoch 8910, Training loss 458.2252\n",
      "Epoch 8920, Training loss 495.1729\n",
      "Epoch 8930, Training loss 478.7651\n",
      "Epoch 8940, Training loss 464.1862\n",
      "Epoch 8950, Training loss 461.7005\n",
      "Epoch 8960, Training loss 457.6655\n",
      "Epoch 8970, Training loss 458.1389\n",
      "Epoch 8980, Training loss 457.5039\n",
      "Epoch 8990, Training loss 457.4812\n",
      "Epoch 9000, Training loss 457.4403\n",
      "Epoch 9010, Training loss 457.3811\n",
      "Epoch 9020, Training loss 457.3457\n",
      "Epoch 9030, Training loss 457.3153\n",
      "Epoch 9040, Training loss 457.2852\n",
      "Epoch 9050, Training loss 457.2552\n",
      "Epoch 9060, Training loss 457.2248\n",
      "Epoch 9070, Training loss 457.1944\n",
      "Epoch 9080, Training loss 457.1639\n",
      "Epoch 9090, Training loss 457.1334\n",
      "Epoch 9100, Training loss 457.1035\n",
      "Epoch 9110, Training loss 457.1029\n",
      "Epoch 9120, Training loss 459.8014\n",
      "Epoch 9130, Training loss 568.7750\n",
      "Epoch 9140, Training loss 457.7161\n",
      "Epoch 9150, Training loss 460.9328\n",
      "Epoch 9160, Training loss 460.8374\n",
      "Epoch 9170, Training loss 458.1530\n",
      "Epoch 9180, Training loss 456.9160\n",
      "Epoch 9190, Training loss 456.9796\n",
      "Epoch 9200, Training loss 456.8828\n",
      "Epoch 9210, Training loss 456.8082\n",
      "Epoch 9220, Training loss 456.7803\n",
      "Epoch 9230, Training loss 456.7477\n",
      "Epoch 9240, Training loss 456.7161\n",
      "Epoch 9250, Training loss 456.6877\n",
      "Epoch 9260, Training loss 456.6586\n",
      "Epoch 9270, Training loss 456.6295\n",
      "Epoch 9280, Training loss 456.6003\n",
      "Epoch 9290, Training loss 456.5711\n",
      "Epoch 9300, Training loss 456.5419\n",
      "Epoch 9310, Training loss 456.5126\n",
      "Epoch 9320, Training loss 456.4830\n",
      "Epoch 9330, Training loss 456.4541\n",
      "Epoch 9340, Training loss 456.4402\n",
      "Epoch 9350, Training loss 457.0694\n",
      "Epoch 9360, Training loss 497.1825\n",
      "Epoch 9370, Training loss 478.3217\n",
      "Epoch 9380, Training loss 462.9035\n",
      "Epoch 9390, Training loss 459.3538\n",
      "Epoch 9400, Training loss 456.8530\n",
      "Epoch 9410, Training loss 456.5948\n",
      "Epoch 9420, Training loss 456.4162\n",
      "Epoch 9430, Training loss 456.1855\n",
      "Epoch 9440, Training loss 456.1690\n",
      "Epoch 9450, Training loss 456.1382\n",
      "Epoch 9460, Training loss 456.1045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9470, Training loss 456.0733\n",
      "Epoch 9480, Training loss 456.0440\n",
      "Epoch 9490, Training loss 456.0153\n",
      "Epoch 9500, Training loss 455.9869\n",
      "Epoch 9510, Training loss 455.9585\n",
      "Epoch 9520, Training loss 455.9297\n",
      "Epoch 9530, Training loss 455.9010\n",
      "Epoch 9540, Training loss 455.8723\n",
      "Epoch 9550, Training loss 455.8488\n",
      "Epoch 9560, Training loss 456.7629\n",
      "Epoch 9570, Training loss 596.1927\n",
      "Epoch 9580, Training loss 456.5527\n",
      "Epoch 9590, Training loss 456.6831\n",
      "Epoch 9600, Training loss 455.8610\n",
      "Epoch 9610, Training loss 455.7357\n",
      "Epoch 9620, Training loss 455.6982\n",
      "Epoch 9630, Training loss 455.6898\n",
      "Epoch 9640, Training loss 455.6769\n",
      "Epoch 9650, Training loss 455.6402\n",
      "Epoch 9660, Training loss 455.5860\n",
      "Epoch 9670, Training loss 455.5490\n",
      "Epoch 9680, Training loss 455.5240\n",
      "Epoch 9690, Training loss 455.4968\n",
      "Epoch 9700, Training loss 455.4702\n",
      "Epoch 9710, Training loss 455.4437\n",
      "Epoch 9720, Training loss 455.4172\n",
      "Epoch 9730, Training loss 455.3906\n",
      "Epoch 9740, Training loss 455.3640\n",
      "Epoch 9750, Training loss 455.3372\n",
      "Epoch 9760, Training loss 455.3104\n",
      "Epoch 9770, Training loss 455.2835\n",
      "Epoch 9780, Training loss 455.2565\n",
      "Epoch 9790, Training loss 455.2294\n",
      "Epoch 9800, Training loss 455.2023\n",
      "Epoch 9810, Training loss 455.1750\n",
      "Epoch 9820, Training loss 455.1477\n",
      "Epoch 9830, Training loss 455.1203\n",
      "Epoch 9840, Training loss 455.0928\n",
      "Epoch 9850, Training loss 455.0663\n",
      "Epoch 9860, Training loss 455.3686\n",
      "Epoch 9870, Training loss 542.4547\n",
      "Epoch 9880, Training loss 499.2819\n",
      "Epoch 9890, Training loss 467.3529\n",
      "Epoch 9900, Training loss 456.3186\n",
      "Epoch 9910, Training loss 454.9379\n",
      "Epoch 9920, Training loss 454.9815\n",
      "Epoch 9930, Training loss 454.9463\n",
      "Epoch 9940, Training loss 454.8821\n",
      "Epoch 9950, Training loss 454.8397\n",
      "Epoch 9960, Training loss 454.8122\n",
      "Epoch 9970, Training loss 454.7894\n",
      "Epoch 9980, Training loss 454.7647\n",
      "Epoch 9990, Training loss 454.7383\n",
      "Epoch 10000, Training loss 454.7128\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "opt = optim.Adam(nn_arch.parameters(), lr=1e-3)\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # feed the training features to the network.\n",
    "    nn_output = nn_arch(train_features_tn)\n",
    "    # Compute the loss with respect to the ground truth.\n",
    "    loss = nn.MSELoss()(nn_output, train_target_tn)\n",
    "\n",
    "    # Reset the gradient\n",
    "    opt.zero_grad()\n",
    "    # Backpropagate and update the weights\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': nn_arch.state_dict(),\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        'loss': loss.item(),\n",
    "        }, 'best_checkpoint.pt')\n",
    "        best_loss = loss.item()\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49686f",
   "metadata": {},
   "source": [
    "Testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dd27822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.290215365063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  2.7739],\n",
       "        [  4.3585],\n",
       "        [  9.0754],\n",
       "        ...,\n",
       "        [-20.8843],\n",
       "        [-26.0388],\n",
       "        [-12.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_arch.eval()\n",
    "test_output = nn_arch(test_features_tn)\n",
    "loss = nn.MSELoss()(test_output, test_target_tn)\n",
    "print(loss.item())\n",
    "test_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
